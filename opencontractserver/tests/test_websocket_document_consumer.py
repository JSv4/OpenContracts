from __future__ import annotations

import json
import logging
from typing import Any
from urllib.parse import quote

import vcr
from channels.db import database_sync_to_async
from channels.testing import WebsocketCommunicator
from django.conf import settings
from django.db.models import Q
from django.test.utils import override_settings
from graphql_relay import to_global_id

from opencontractserver.conversations.models import ChatMessage, Conversation  # noqa
from opencontractserver.llms.agents import for_document
from opencontractserver.llms.types import AgentFramework
from opencontractserver.tests.base import WebsocketFixtureBaseTestCase

logging.basicConfig()
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
vcr_log = logging.getLogger("vcr")
vcr_log.setLevel(logging.WARNING)


@override_settings(USE_AUTH0=False)
class DocumentConversationWebsocketTestCase(WebsocketFixtureBaseTestCase):
    """
    End-to-end websocket test for the refactored DocumentQueryConsumer.
    Tests both new and loaded conversations, and a two-turn interaction.
    """

    maxDiff = None

    # Expected responses for new (not loaded from history) conversations
    expected_responses_new = {
        AgentFramework.LLAMA_INDEX.value: [
            """## Document Summary

### Title: USC Title 1 - Chapter 1

This document contains an autogenerated test summary. Please replace it with real content if needed.

If you need more specific information or details from the document, feel free to ask!""",  # noqa: E501
            """The document titled **USC Title 1 - Chapter 1** has a brief summary, currently consisting of only 15 tokens. This indicates that the content is quite limited and likely serves as a placeholder.

If you need further details or specific sections summarized, please let me know!""",  # noqa: E501
        ],
        AgentFramework.PYDANTIC_AI.value: [
            """I can't stream content directly, but I can help you find something to watch or listen to! What type of content are you interested in? Movies, TV shows, music, or something else? Let me know your preferences!""",  # noqa: E501
            """I would be happy to help with that! However, I currently don't have access to external documents or files. If you can provide the text or main points from the document, I can summarize it for you.""",  # noqa: E501
        ],
    }

    # Expected responses for conversations loaded from history
    expected_responses_loaded = {
        AgentFramework.LLAMA_INDEX.value: [
            """# Summary of USC Title 1 - Chapter 1

Autogenerated test summary – replace with real content if needed.""",  # noqa: E501
            """The summary for USC Title 1 - Chapter 1 is currently an autogenerated test summary. It suggests that the content should be replaced with more detailed and specific information if needed.""",  # noqa: E501
        ],
        AgentFramework.PYDANTIC_AI.value: [
            """It seems like you're requesting a streaming service or content. However, I can't stream media directly. I can help you find recommendations for movies, shows, or music if you'd like! What type of content are you interested in?""",  # noqa: E501
            """I'd be happy to help summarize the document! However, I don't have access to it. If you could provide the main points or sections of the document, I can help create a summary based on that information.""",  # noqa: E501
        ],
    }

    async def _assert_streaming_flow(
        self,
        communicator: WebsocketCommunicator,
        query_text: str,
        expected_response_key: str,
        is_loaded_conversation: bool = False,  # For logging purposes
    ) -> str:
        """
        Sends a query on an existing communicator and verifies the streaming response flow.
        """
        if is_loaded_conversation:
            logger.info(
                f"[_assert_streaming_flow] Testing with LOADED conversation. Query: '{query_text}'"
            )
        else:
            logger.info(
                f"[_assert_streaming_flow] Testing with NEW/ANONYMOUS conversation. Query: '{query_text}'"
            )

        # ------------------------------------------------------------------
        # Log outgoing user message (for clarity in CI logs)
        # ------------------------------------------------------------------
        current_framework_setting = str(
            getattr(settings, "LLMS_DOCUMENT_AGENT_FRAMEWORK", "llama_index")
        )
        logger.info(
            f"[TEST][framework={current_framework_setting}][loaded={is_loaded_conversation}] USER → '{query_text}'"
        )

        await communicator.send_to(json.dumps({"query": query_text}))

        received: list[dict[str, Any]] = []
        while True:
            try:
                msg = await communicator.receive_from(
                    timeout=50
                )  # Increased timeout for potentially longer summaries
            except Exception as e:
                self.fail(
                    f"Timed-out waiting for websocket messages for query '{query_text}': {e}"
                )

            payload = json.loads(msg)
            logger.debug(f"Payload for query '{query_text}': {payload}")
            received.append(payload)

            if payload.get("type") == "ASYNC_FINISH":
                break

        self.assertGreaterEqual(
            len(received),
            3,
            f"For query '{query_text}', consumer should emit at least START, one CONTENT, FINISH.",
        )
        self.assertEqual(
            received[0]["type"],
            "ASYNC_START",
            f"First message for '{query_text}' should be ASYNC_START",
        )
        self.assertEqual(
            received[-1]["type"],
            "ASYNC_FINISH",
            f"Last message for '{query_text}' should be ASYNC_FINISH",
        )

        content_msgs = [m for m in received if m["type"] == "ASYNC_CONTENT"]
        self.assertTrue(
            content_msgs,
            f"At least one ASYNC_CONTENT expected for query '{query_text}'.",
        )

        full_text = "".join(msg["content"] for msg in content_msgs).strip()
        logger.info(
            f"[_assert_streaming_flow] Full LLM response text for query '{query_text}': {full_text}"
        )

        # Log assistant response for easier debugging in CI logs
        logger.info(
            f"[TEST][framework={current_framework_setting}][loaded={is_loaded_conversation}] ASSISTANT ← '{full_text}'"
        )

        # The expected_response_key ("query1_response" or "query2_response") is used directly
        # with the selected dictionary.

        start_msg_id = received[0]["data"]["message_id"]
        self.assertTrue(
            start_msg_id, f"START message for '{query_text}' must contain a message_id."
        )

        for msg in received[1:]:
            if "data" in msg and "message_id" in msg["data"]:
                self.assertEqual(
                    msg["data"]["message_id"],
                    start_msg_id,
                    f"message_id for '{query_text}' must remain constant.",
                )

        return full_text

    async def _create_and_populate_conversation(self) -> Conversation:
        """Helper to create a conversation with a couple of messages."""
        conversation = await Conversation.objects.acreate(
            title="Pre-existing Test Conversation",
            creator=self.user,
            chat_with_document=self.doc,
        )
        await ChatMessage.objects.acreate(
            conversation=conversation,
            msg_type="HUMAN",
            content="This is a previous user message about general topics.",
            creator=self.user,
        )
        await ChatMessage.objects.acreate(
            conversation=conversation,
            msg_type="LLM",
            content="Acknowledged. I am a helpful assistant.",
            creator=self.user,  # LLM messages are also created by the user in current model
        )
        logger.info(
            f"[_create_and_populate_conversation] Created "
            f"Conversation ID: {conversation.id} with 2 messages "
            f"for Document ID: {self.doc.id}"
        )
        return conversation

    @vcr.use_cassette(
        "fixtures/vcr_cassettes/test_document_conversation_ws.yaml",
        filter_headers=["authorization"],
        record_mode="once",  # Change to "rewrite" or remove cassette to re-record
    )
    async def test_multiturn_streaming_flow__all_default_frameworks(self) -> None:
        """
        Tests a two-turn streaming flow for new conversations.
        """
        print("--------------------------------")
        print(
            "\n\nXOXO - [_test_multiturn_streaming_flow__all_default_frameworks] "
            "Tests a two-turn streaming flow for new conversations."
        )
        print("--------------------------------")

        for framework in ("llama_index", "pydantic_ai"):
            print(f"XOXO: {framework}")
            with self.subTest(default_framework=framework):
                with override_settings(
                    LLMS_DEFAULT_AGENT_FRAMEWORK=framework,
                    LLMS_DOCUMENT_AGENT_FRAMEWORK=framework,
                    LLMS_CORPUS_AGENT_FRAMEWORK=framework,
                ):
                    await self._run_full_conversation_flow(framework)

    @vcr.use_cassette(
        "fixtures/vcr_cassettes/test_document_conversation_ws_loaded.yaml",
        filter_headers=["authorization"],
        record_mode="once",
    )
    async def test_multiturn_streaming_flow_with_loaded_conversation__all_default_frameworks(
        self,
    ) -> None:
        """
        Tests a two-turn streaming flow when loading an existing conversation.
        """
        print("--------------------------------")
        print(
            "\n\nXOXO - [_test_multiturn_streaming_flow_with_loaded_conversation] "
            "Tests a two-turn streaming flow when loading an existing conversation."
        )
        print("--------------------------------")

        for framework in ("llama_index", "pydantic_ai"):
            print(f"XOXO: {framework}")
            with self.subTest(default_framework=framework):
                with override_settings(
                    LLMS_DEFAULT_AGENT_FRAMEWORK=framework,
                    LLMS_DOCUMENT_AGENT_FRAMEWORK=framework,
                    LLMS_CORPUS_AGENT_FRAMEWORK=framework,
                ):
                    await self._run_loaded_conversation_flow(framework)

    # --- Negative-path helpers and tests remain unchanged from your previous version ---
    async def _assert_invalid_token(self) -> None:
        """Connection should be rejected (code 4000) when the JWT is invalid."""
        graphql_id = to_global_id("DocumentType", self.doc.id)
        encoded_graphql_id = quote(graphql_id)
        encoded_corpus_id = quote(to_global_id("CorpusType", self.corpus.id))

        communicator = WebsocketCommunicator(
            self.application,
            f"ws/document/{encoded_graphql_id}/query/"
            f"corpus/{encoded_corpus_id}/?token=not_a_real_token",
        )
        connected, close_code = await communicator.connect()
        self.assertFalse(connected)
        self.assertEqual(close_code, 4000)

    async def _assert_missing_token(self) -> None:
        """Omitting the token entirely must also yield close 4000."""
        graphql_id = to_global_id("DocumentType", self.doc.id)
        encoded_graphql_id = quote(graphql_id)
        encoded_corpus_id = quote(to_global_id("CorpusType", self.corpus.id))

        communicator = WebsocketCommunicator(
            self.application,
            f"ws/document/{encoded_graphql_id}/query/" f"corpus/{encoded_corpus_id}/",
        )
        connected, close_code = await communicator.connect()
        self.assertFalse(connected)
        self.assertEqual(close_code, 4000)

    async def _assert_invalid_document(self) -> None:
        """
        A non-existent document ID should result in:
        • WebSocket *accepted*
        • Immediate `SYNC_CONTENT` error payload
        • Close code 4000
        """
        bad_doc_gid = to_global_id("DocumentType", 999_999)
        encoded_bad_doc = quote(bad_doc_gid)
        encoded_corpus_id = quote(to_global_id("CorpusType", self.corpus.id))

        communicator = WebsocketCommunicator(
            self.application,
            f"ws/document/{encoded_bad_doc}/query/"
            f"corpus/{encoded_corpus_id}/?token={self.token}",
        )
        connected, _ = await communicator.connect()
        self.assertTrue(connected)

        raw = await communicator.receive_from(timeout=50)
        payload = json.loads(raw)
        self.assertEqual(payload["type"], "SYNC_CONTENT")
        self.assertIn("error", payload["data"])
        self.assertEqual(payload["data"]["error"], "Requested Document not found.")

        # The consumer should now close the websocket with code 4000.
        close_event = await communicator.receive_output(timeout=50)
        self.assertEqual(close_event["type"], "websocket.close")
        self.assertEqual(close_event["code"], 4000)

        # Ensure the communicator is fully shut down
        await communicator.wait()

    async def test_invalid_token(self) -> None:  # noqa: D401
        """Connection rejected with an **invalid** JWT token."""
        await self._assert_invalid_token()

    async def test_missing_token(self) -> None:  # noqa: D401
        """Connection rejected when **no** JWT token is supplied."""
        await self._assert_missing_token()

    async def test_invalid_document_id(self) -> None:  # noqa: D401
        """Proper SYNC_CONTENT error for a non-existent document ID."""
        await self._assert_invalid_document()

    # ------------------------------------------------------------------ #
    # Utility helpers for conversation-history inspection / assertions   #
    # ------------------------------------------------------------------ #

    async def _fetch_last_conversation(self) -> Conversation:
        """
        Returns the most-recent conversation started by *this* test user
        for the current document.
        """
        return await (
            Conversation.objects.filter(creator=self.user, chat_with_document=self.doc)
            .order_by("-created_at")
            .afirst()
        )

    async def _log_and_assert_history(
        self,
        conversation: Conversation,
        expected_queries: list[str],
        expected_llm_replies: list[str],
        expect_prepopulated_messages: bool = False,
    ) -> None:
        """
        1. Dumps the complete message history to the test log.
        2. Performs a few high-level assertions to guarantee that the agent
           actually *stored* what was exchanged over the websocket.

        Args:
            conversation:   The Conversation instance to inspect.
            expected_queries:  The HUMAN contents we sent in the test.
            expected_llm_replies:  The LLM responses we asserted on.
        """
        # Pull messages – oldest first
        messages = await database_sync_to_async(list)(
            ChatMessage.objects.filter(conversation=conversation)
            .order_by("created_at")
            .all()
        )

        # --- Pretty print to console / CI logs -------------------------
        logger.info("\n====== Conversation %s – full history ======", conversation.pk)
        for msg in messages:
            logger.info(
                "[%s] %s: %s",
                msg.created_at.isoformat(timespec="seconds"),
                msg.msg_type,
                msg.content.replace("\n", " ")[:200]
                + ("…" if len(msg.content) > 200 else ""),
            )
        logger.info("============================================================\n")

        # --- Basic structural checks -----------------------------------
        # For a *new* conversation we expect 2 HUMAN + 2 LLM messages
        # For a *loaded* conversation we already pre-populated 2 messages.
        expected_msg_count = len(expected_queries) + len(expected_llm_replies)
        self.assertEqual(
            len(messages),
            expected_msg_count + (2 if expect_prepopulated_messages else 0),
            "Unexpected number of messages stored for conversation %s"
            % conversation.pk,
        )

        # # Assert the queries were stored exactly as HUMAN messages
        # human_contents = [m.content.strip() for m in messages if m.msg_type == "HUMAN"]
        # self.assertEqual(human_contents, expected_queries)

        # # Assert LLM replies were stored exactly
        # llm_contents = [m.content.strip() for m in messages if m.msg_type == "LLM"]
        # self.assertEqual(llm_contents, expected_llm_replies)

    # --- Start of new helper method ---
    async def _run_full_conversation_flow(self, framework: str) -> Conversation:
        """Runs the full new conversation flow and returns the conversation."""
        graphql_doc_id = to_global_id("DocumentType", self.doc.id)
        encoded_graphql_doc_id = quote(graphql_doc_id)
        encoded_corpus_id = quote(to_global_id("CorpusType", self.corpus.id))
        ws_path = f"ws/document/{encoded_graphql_doc_id}/query/corpus/{encoded_corpus_id}/?token={self.token}"

        communicator = WebsocketCommunicator(self.application, ws_path)
        connected, _ = await communicator.connect()
        self.assertTrue(connected, "WebSocket for new conversation should connect.")

        response_1 = await self._assert_streaming_flow(
            communicator=communicator,
            query_text="Please stream something",
            expected_response_key="query1_response",
            is_loaded_conversation=False,
        )
        response_2 = await self._assert_streaming_flow(
            communicator=communicator,
            query_text="Ok, please summarize the document.",
            expected_response_key="query2_response",
            is_loaded_conversation=False,
        )

        self.assertEqual(
            [response_1, response_2], self.expected_responses_new[framework]
        )

        await communicator.disconnect()

        conversation = await self._fetch_last_conversation()
        await self._log_and_assert_history(
            conversation,
            expected_queries=[
                "Please stream something",
                "Ok, please summarize the document.",
            ],
            expected_llm_replies=[response_1, response_2],
        )
        return conversation

    # --- End of new helper method ---

    # --- Start of new helper method for loaded conversation ---
    async def _run_loaded_conversation_flow(self, framework: str) -> Conversation:
        """Runs the full loaded conversation flow and returns the conversation."""
        conversation = await self._create_and_populate_conversation()

        history_check_agent = await for_document(
            document=self.doc,
            corpus=self.corpus,
            user_id=self.user.id,
            conversation_id=conversation.id,
        )
        actual_history_for_log = await history_check_agent.get_conversation_messages()
        print(
            f"Fetched {len(actual_history_for_log)} messages via agent API for "
            f"conversation {conversation.id} for logging."
        )

        graphql_doc_id = to_global_id("DocumentType", self.doc.id)
        encoded_graphql_doc_id = quote(graphql_doc_id)
        encoded_corpus_id = quote(to_global_id("CorpusType", self.corpus.id))
        graphql_convo_id = to_global_id("ConversationType", conversation.id)
        encoded_graphql_convo_id = quote(graphql_convo_id)
        ws_path = f"ws/document/{encoded_graphql_doc_id}/query/corpus/{encoded_corpus_id}/?token={self.token}&load_from_conversation_id={encoded_graphql_convo_id}"  # noqa: E501

        communicator = WebsocketCommunicator(self.application, ws_path)
        connected, _ = await communicator.connect()
        self.assertTrue(connected, "WebSocket for loaded conversation should connect.")

        response_1 = await self._assert_streaming_flow(
            communicator=communicator,
            query_text="Please stream something",
            expected_response_key="query1_response",
            is_loaded_conversation=True,
        )
        response_2 = await self._assert_streaming_flow(
            communicator=communicator,
            query_text="Ok, please summarize the document.",
            expected_response_key="query2_response",
            is_loaded_conversation=True,
        )
        self.assertEqual(
            [response_1, response_2],
            self.expected_responses_loaded[framework],
        )

        await communicator.disconnect()

        # The conversation object 'conversation' here refers to the one created by _create_and_populate_conversation.
        # Messages are added to this existing conversation.
        await self._log_and_assert_history(
            conversation,  # Use the existing conversation object
            expected_queries=[
                "Please stream something",
                "Ok, please summarize the document.",
            ],
            expected_llm_replies=[response_1, response_2],
            expect_prepopulated_messages=True,
        )
        return conversation

    # --- End of new helper method for loaded conversation ---


class ConversationSourceLoggingTestCase(DocumentConversationWebsocketTestCase):
    """
    Verifies that LLM replies written during the websocket flow
    have their `sources` persisted in the Message.metadata column.
    """

    async def _run_full_conversation_flow(self, framework: str) -> Conversation:
        """Override parent method with retrieval-oriented prompts to ensure sources are generated."""
        graphql_doc_id = to_global_id("DocumentType", self.doc.id)
        encoded_graphql_doc_id = quote(graphql_doc_id)
        encoded_corpus_id = quote(to_global_id("CorpusType", self.corpus.id))
        ws_path = (
            f"ws/document/{encoded_graphql_doc_id}/query/"
            f"corpus/{encoded_corpus_id}/?token={self.token}"
        )

        communicator = WebsocketCommunicator(self.application, ws_path)
        connected, _ = await communicator.connect()
        self.assertTrue(connected, "WebSocket for new conversation should connect.")

        # Use retrieval-oriented queries that will trigger vector store search
        response_1 = await self._assert_streaming_flow(
            communicator=communicator,
            query_text="Regarding US Code Title 1, what is the rule of construction for 'words importing the masculine gender'?",  # noqa: E501
            expected_response_key="query1_response",
            is_loaded_conversation=False,
        )
        response_2 = await self._assert_streaming_flow(
            communicator=communicator,
            query_text="What does US Code Title 1, Section 3 state about the term 'vessel'?",
            expected_response_key="query2_response",
            is_loaded_conversation=False,
        )

        await communicator.disconnect()

        conversation = await self._fetch_last_conversation()
        # Don't assert on specific response content since we only care about sources
        await self._log_and_assert_history(
            conversation,
            expected_queries=[
                "Regarding US Code Title 1, what is the rule of construction for 'words importing the masculine gender'?",  # noqa: E501
                "What does US Code Title 1, Section 3 state about the term 'vessel'?",
            ],
            expected_llm_replies=[response_1, response_2],
        )
        return conversation

    async def _run_loaded_conversation_flow(self, framework: str) -> Conversation:
        """Override parent method with retrieval-oriented prompts for loaded conversations."""
        conversation = await self._create_and_populate_conversation()

        history_check_agent = await for_document(
            document=self.doc,
            corpus=self.corpus,
            user_id=self.user.id,
            conversation_id=conversation.id,
        )
        actual_history_for_log = await history_check_agent.get_conversation_messages()
        print(
            f"Fetched {len(actual_history_for_log)} messages via agent API for "
            f"conversation {conversation.id} for logging."
        )

        graphql_doc_id = to_global_id("DocumentType", self.doc.id)
        encoded_graphql_doc_id = quote(graphql_doc_id)
        encoded_corpus_id = quote(to_global_id("CorpusType", self.corpus.id))
        graphql_convo_id = to_global_id("ConversationType", conversation.id)
        encoded_graphql_convo_id = quote(graphql_convo_id)
        ws_path = (
            f"ws/document/{encoded_graphql_doc_id}/query/corpus/{encoded_corpus_id}/"
            f"?token={self.token}&load_from_conversation_id={encoded_graphql_convo_id}"
        )

        communicator = WebsocketCommunicator(self.application, ws_path)
        connected, _ = await communicator.connect()
        self.assertTrue(connected, "WebSocket for loaded conversation should connect.")

        # Use retrieval-oriented queries
        response_1 = await self._assert_streaming_flow(
            communicator=communicator,
            query_text="Regarding US Code Title 1, what is the rule of construction for 'words importing the masculine gender'?",  # noqa: E501
            expected_response_key="query1_response",
            is_loaded_conversation=True,
        )
        response_2 = await self._assert_streaming_flow(
            communicator=communicator,
            query_text="What does US Code Title 1, Section 3 state about the term 'vessel'?",
            expected_response_key="query2_response",
            is_loaded_conversation=True,
        )

        await communicator.disconnect()

        await self._log_and_assert_history(
            conversation,
            expected_queries=[
                "Regarding US Code Title 1, what is the rule of construction for 'words importing the masculine gender'?",  # noqa: E501
                "What does US Code Title 1, Section 3 state about the term 'vessel'?",
            ],
            expected_llm_replies=[response_1, response_2],
            expect_prepopulated_messages=True,
        )
        return conversation

    async def _assert_sources_persisted(self, conversation: Conversation) -> None:
        # Fetch only LLM messages created in the conversation
        llm_messages = await database_sync_to_async(
            lambda: list(
                conversation.chat_messages.filter(msg_type="LLM").exclude(
                    Q(data__sources=None) | Q(data__sources=[])
                )
            )
        )()
        self.assertTrue(
            llm_messages,
            "Expected at least one LLM message with non-empty `data['sources']`",
        )

        # Make sure every retrieved message has a sources list with at least one element
        for msg in llm_messages:
            sources = msg.data["sources"]
            self.assertIsInstance(sources, list)
            self.assertGreater(
                len(sources), 0, f"Message {msg.id} has empty sources list"
            )
            # Optional: check shape of first source
            first = sources[0]
            self.assertIn("content", first)
            self.assertIn("similarity_score", first)

    async def _test_sources_for_framework(
        self, framework: str, cassette_name: str, test_type: str
    ) -> None:
        """Helper method to test sources for a specific framework."""

        @vcr.use_cassette(
            cassette_name,
            filter_headers=["authorization"],
            record_mode="once",
        )
        async def run_test():
            with override_settings(
                LLMS_DEFAULT_AGENT_FRAMEWORK=framework,
                LLMS_DOCUMENT_AGENT_FRAMEWORK=framework,
                LLMS_CORPUS_AGENT_FRAMEWORK=framework,
            ):
                if test_type == "new":
                    conversation = await self._run_full_conversation_flow(framework)
                elif test_type == "loaded":
                    conversation = await self._run_loaded_conversation_flow(framework)
                else:
                    raise ValueError(f"Unknown test_type: {test_type}")

                await self._assert_sources_persisted(conversation)

        await run_test()

    @vcr.use_cassette(
        "fixtures/vcr_cassettes/test_document_conversation_sources_ws.yaml",
        filter_headers=["authorization"],
        record_mode="once",
    )
    async def test_sources_are_logged_for_new_conversation(self) -> None:
        """Test sources are logged for new conversation across all frameworks."""
        for framework in ("llama_index", "pydantic_ai"):
            with self.subTest(framework=framework):
                await self._test_sources_for_framework(
                    framework=framework,
                    cassette_name=f"fixtures/vcr_cassettes/test_document_conversation_sources_ws_{framework}.yaml",
                    test_type="new",
                )

    @vcr.use_cassette(
        "fixtures/vcr_cassettes/test_document_conversation_sources_ws_loaded.yaml",
        filter_headers=["authorization"],
        record_mode="once",
    )
    async def test_sources_are_logged_for_loaded_conversation(self) -> None:
        """Test sources are logged for loaded conversation across all frameworks."""
        for framework in ("llama_index", "pydantic_ai"):
            with self.subTest(framework=framework):
                await self._test_sources_for_framework(
                    framework=framework,
                    cassette_name=f"fixtures/vcr_cassettes/test_document_conversation_sources_ws_loaded_{framework}.yaml",  # noqa: E501
                    test_type="loaded",
                )
