{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OpenContracts \u00b6 The Free and Open Source Document Analysis Platform \u00b6 CI/CD Meta What Does it Do? \u00b6 OpenContracts is an Apache-2 Licensed software application to label, share and search annotate documents. It's designed specifically to label documents with complex layouts such as contracts, scientific papers, newspapers, etc. When combine with a NLP processing engine like Gremlin Engine (another of our open source projects), OpenContracts not only lets humans collaborate on and share document annotations, it also can analyze and export data from contracts using state-of-the-art NLP technology. Why Does it Exist? \u00b6 The OpenContracts stack is designed to provide a cutting edge frontend experience while providing access to the incredible machine learning and natural language processing capabilities of Python. For this reason, our frontend is based on React. We use a GraphQL API to connect it to a django-based backend. Django is a incredibly mature, battle-tested framework that is written in Python, so integrating all the amazing Python-based AI and NLP libraries out there is super easy. We'd like to give credit to AllenAI's PAWLs project for our document annotating component. We rewrote most of the code base and replaced their backend entirely, so it was hard to keep , but we believe in giving credit where it's due! We are relying on their document parser, however, as it produces a really excellent text and x-y coordinate layer that we'd encourage others to use as well in similar applications that require you to interact with complex text layouts. Limitations \u00b6 At the moment, it only works with PDFs. In the future, it will be able to convert other document types to PDF for storage and labeling. PDF is an excellent format for this as it introduces a consistent, repeatable format which we can use to generate a text and x-y coordinate layer from scratch. Formats like .docx and .html are too complex and varied to provide an easy, consistent format. Likewise, the output quality of many converters and tools is sub-par and these tools can produce very different document structures for the same inputs. About OpenSource.Legal \u00b6 OpenSource.Legal believes that the effective, digital transformation of the legal services industry and the execution of \"the law\", broadly speaking, requires shared solutions and tools to solve some of the problems that are common to almost every legal workflow. The current splintering of service delivery into dozens of incompatible platforms with limited configurations threatens to put software developers and software vendors in the driver seat of the industry. We firmly believe that lawyers and legal engineers, armed with easily configurable and extensible tools can much more effectively design the workflows and user experiences that they need to deliver and scale their expertise. Visit us at https://opensource.legal for a directory of open source legal projects and an overview of our projects.","title":"About"},{"location":"#opencontracts","text":"","title":"OpenContracts"},{"location":"#the-free-and-open-source-document-analysis-platform","text":"CI/CD Meta","title":"The Free and Open Source Document Analysis Platform"},{"location":"#what-does-it-do","text":"OpenContracts is an Apache-2 Licensed software application to label, share and search annotate documents. It's designed specifically to label documents with complex layouts such as contracts, scientific papers, newspapers, etc. When combine with a NLP processing engine like Gremlin Engine (another of our open source projects), OpenContracts not only lets humans collaborate on and share document annotations, it also can analyze and export data from contracts using state-of-the-art NLP technology.","title":"What Does it Do?"},{"location":"#why-does-it-exist","text":"The OpenContracts stack is designed to provide a cutting edge frontend experience while providing access to the incredible machine learning and natural language processing capabilities of Python. For this reason, our frontend is based on React. We use a GraphQL API to connect it to a django-based backend. Django is a incredibly mature, battle-tested framework that is written in Python, so integrating all the amazing Python-based AI and NLP libraries out there is super easy. We'd like to give credit to AllenAI's PAWLs project for our document annotating component. We rewrote most of the code base and replaced their backend entirely, so it was hard to keep , but we believe in giving credit where it's due! We are relying on their document parser, however, as it produces a really excellent text and x-y coordinate layer that we'd encourage others to use as well in similar applications that require you to interact with complex text layouts.","title":"Why Does it Exist?"},{"location":"#limitations","text":"At the moment, it only works with PDFs. In the future, it will be able to convert other document types to PDF for storage and labeling. PDF is an excellent format for this as it introduces a consistent, repeatable format which we can use to generate a text and x-y coordinate layer from scratch. Formats like .docx and .html are too complex and varied to provide an easy, consistent format. Likewise, the output quality of many converters and tools is sub-par and these tools can produce very different document structures for the same inputs.","title":"Limitations"},{"location":"#about-opensourcelegal","text":"OpenSource.Legal believes that the effective, digital transformation of the legal services industry and the execution of \"the law\", broadly speaking, requires shared solutions and tools to solve some of the problems that are common to almost every legal workflow. The current splintering of service delivery into dozens of incompatible platforms with limited configurations threatens to put software developers and software vendors in the driver seat of the industry. We firmly believe that lawyers and legal engineers, armed with easily configurable and extensible tools can much more effectively design the workflows and user experiences that they need to deliver and scale their expertise. Visit us at https://opensource.legal for a directory of open source legal projects and an overview of our projects.","title":"About OpenSource.Legal"},{"location":"acknowledgements/","text":"OpenContracts is built in part on top of the PAWLs project frontend. We have made extensive changes, however, and plan to remove even more of the original PAWLs codebase, particularly their state management, as it's currently duplucitive of the Apollo state store we use throughout the application. That said, PAWLs was the inspiration for how we handle text extraction, and we're planning to continue using their PDF rendering code. We are also using PAWLs' pre-processing script, which is based on Grobid. We should also thank the Grobid project, which was clearly a source of inspiration for PAWLs and an extremely impressive tool. Grobid is designed more for medical and scientific papers, but, nevertheless, offers a tremendous amount of inspiration and examples for the legal world to borrow. Perhaps there is an opportunity to have a unified tool in that respect. Finally, let's not forget Tesseract , the OCR engine that started its life as an HP research project in the 1980s before being taken over by Google in the early aughts and finally becoming an independent project in 2018. Were it not for the excellent, free OCR provided by Tesseract, we'd have to rely on commercial OCR tech, which would make this kind of opensource, free project prohibitively expensive. Thanks to the many, many people who've made free OCR possible over the nearly 40 years Tesseract has been under development.","title":"Acknowledgements"},{"location":"philosophy/","text":"Don't Repeat Yourself \u00b6 OpenContracts is designed not only be a powerful document analysis and annotation platform, it's also envisioned as a way to embrace the DRY (Don't Repeat Yourself) principle for legal and legal engineering. You can make a corpus, along with all of its labels, documents and annotations \"public\" (currently, you must do this via a GraphQL mutation). Once something is public, it's read-only for everyone other than its original creator. People with read-only access can \"clone\" the corpus to create a private copy of the corpus, its documents and its annotations. They can then edit the annotations, add to them, export them, etc. This lets us work from previous document annotations and re-use labels and training data.","title":"Philosophy"},{"location":"philosophy/#dont-repeat-yourself","text":"OpenContracts is designed not only be a powerful document analysis and annotation platform, it's also envisioned as a way to embrace the DRY (Don't Repeat Yourself) principle for legal and legal engineering. You can make a corpus, along with all of its labels, documents and annotations \"public\" (currently, you must do this via a GraphQL mutation). Once something is public, it's read-only for everyone other than its original creator. People with read-only access can \"clone\" the corpus to create a private copy of the corpus, its documents and its annotations. They can then edit the annotations, add to them, export them, etc. This lets us work from previous document annotations and re-use labels and training data.","title":"Don't Repeat Yourself"},{"location":"quick-start/","text":"Quick Start (For use on your local machine) \u00b6 This guide is for people who want to quickly get started using the application and aren't interested in hosting it online for others to use. You'll get a default, local user with admin access. We recommend you change the user password after completing this tutorial. We assume you're using Linux or Max OS, but you could do this on Windows too, assuming you have docker compose and docker installed. The commands to create directories will be different on Windows, but the git, docker and docker-compose commands should all be the same. Step 1 : Clone this Repo Clone the repository into a local directory of your choice. Here, we assume you are using a folder called source in your user's home directory: $ cd ~ $ mkdir source $ cd source $ git clone https://github.com/JSv4/OpenContracts.git Step 2 : Build the Stack Change into the directory of the repository you just cloned, e.g.: cd OpenContracts Now, you need to build the docker compose stack. IF you are okay with the default username and password, and, most importantly, you are NOT PLANNING TO HOST THE APPLICATION online, the default, local settings are sufficient and no configuration is required. If you want to change the $ docker-compose -f local.yml build Bring up the stack: $ docker-compose -f local.yml up Congrats! You have OpenContracts running. If you go to http://localhost:3000 in your browser, you'll see the login page. You can login with the default username and password. These are set in the environment variable file you can find in the ./.envs/.local/' directory. In that directory, you'll see a file called .django`. Caveats The quick start local config is designed for use on a local machine, not for access over the Internet or a network. It uses the local disk for storage (not AWS), and Django's built-i","title":"Quick-Start"},{"location":"quick-start/#quick-start-for-use-on-your-local-machine","text":"This guide is for people who want to quickly get started using the application and aren't interested in hosting it online for others to use. You'll get a default, local user with admin access. We recommend you change the user password after completing this tutorial. We assume you're using Linux or Max OS, but you could do this on Windows too, assuming you have docker compose and docker installed. The commands to create directories will be different on Windows, but the git, docker and docker-compose commands should all be the same. Step 1 : Clone this Repo Clone the repository into a local directory of your choice. Here, we assume you are using a folder called source in your user's home directory: $ cd ~ $ mkdir source $ cd source $ git clone https://github.com/JSv4/OpenContracts.git Step 2 : Build the Stack Change into the directory of the repository you just cloned, e.g.: cd OpenContracts Now, you need to build the docker compose stack. IF you are okay with the default username and password, and, most importantly, you are NOT PLANNING TO HOST THE APPLICATION online, the default, local settings are sufficient and no configuration is required. If you want to change the $ docker-compose -f local.yml build Bring up the stack: $ docker-compose -f local.yml up Congrats! You have OpenContracts running. If you go to http://localhost:3000 in your browser, you'll see the login page. You can login with the default username and password. These are set in the environment variable file you can find in the ./.envs/.local/' directory. In that directory, you'll see a file called .django`. Caveats The quick start local config is designed for use on a local machine, not for access over the Internet or a network. It uses the local disk for storage (not AWS), and Django's built-i","title":"Quick Start (For use on your local machine)"},{"location":"requirements/","text":"System Requirements \u00b6 You will need Docker and Docker Compose installed to run Open Contracts. We've developed and run the application a Linux x86_64 environment. We haven't tested on Windows, and it's known that celery is not supported on Windows. For this reason, we do not recommend deployment on Windows. If you must run on a Windows machine, consider using a virtual machine or using the Windows Linux Subsystem. If you need help setting up Docker, we recommend Digital Ocean's setup guide . Likewise, if you need assistance setting up Docker Compose, Digital Ocean's guide is excellent.","title":"System Requirements"},{"location":"requirements/#system-requirements","text":"You will need Docker and Docker Compose installed to run Open Contracts. We've developed and run the application a Linux x86_64 environment. We haven't tested on Windows, and it's known that celery is not supported on Windows. For this reason, we do not recommend deployment on Windows. If you must run on a Windows machine, consider using a virtual machine or using the Windows Linux Subsystem. If you need help setting up Docker, we recommend Digital Ocean's setup guide . Likewise, if you need assistance setting up Docker Compose, Digital Ocean's guide is excellent.","title":"System Requirements"},{"location":"under-the-hood/","text":"Data Layers \u00b6 OpenContracts builds on the work that AllenAI did with PAWLs to create a consistent shared source of truth for data labeling and NLP algorithms, regardless of whether they are layout-aware, like LayoutLM or not, like BERT, Spacy or LexNLP. One of the challenges with natural language documents, particularly contracts is there are so many ways to structure any given file (e.g. .docx or .pdf) to represent exactly the same text. Even an identical document with identical formatting in a format like .pdf can have a significantly different file structure depending on what software was used to create it, the user's choices, and the software's own choices in deciding how to structure its output. PAWLs and OpenContracts attempt to solve this by sending every document through a processing pipeline that provides a uniform and consistent way of extracting and structuring text and layout information. Using the parsing engine of Grobid and the open source OCR engine Tesseract , every single document is re-OCRed (to produce a consistent output for the same inputs) and then the \"tokens\" (text surrounded on all sides by whitespace - typically a word) in the OCRed document are stored as JSONs with their page and positional information. In OpenContracts, we refer to this JSON layer that combines text and positional data as the \"PAWLs\" layer. We use the PAWLs layer to build the full text extract from the document as well and store this as the \"text layer\". Thus, in OpenContracts, every document has three files associated with it - the original pdf, a json file (the \"PAWLs layer\"), and a text file (the \"text layer\"). Because the text layer is built from the PAWLs layer, we can easily translate back and forth from text to positional information - e.g. given the start and end of a span of text the text layer, we can accurately say which PAWLs tokens the span includes, and, based on that, the x,y position of the span in the document. This lets us take the outputs of many NLP libraries which typically produce only start and stop ranges and layer them perfectly on top of the original pdf. With the PAWLs tokens as the source of truth, we can seamlessly transition from text only to layout-aware text. Limitations \u00b6 OCR is not perfect. By only accepting pdf inputs and OCRing every document, we do ignore any text embedded in the pdf. To the extent that text was exported accurately from whatever tool was used to write the document, this introduces some potential loss of fidelity - e.g. if you've ever seen an OCR engine mistake an 'O' or a 0 or 'I' for a '1' or something like that. Typically, however, the instance of such errors is fairly small, and it's a price we have to pay for the power of being able to effortlessly layer NLP outputs that have no layout awareness on top of complex, visual layouts.","title":"How It Works"},{"location":"under-the-hood/#data-layers","text":"OpenContracts builds on the work that AllenAI did with PAWLs to create a consistent shared source of truth for data labeling and NLP algorithms, regardless of whether they are layout-aware, like LayoutLM or not, like BERT, Spacy or LexNLP. One of the challenges with natural language documents, particularly contracts is there are so many ways to structure any given file (e.g. .docx or .pdf) to represent exactly the same text. Even an identical document with identical formatting in a format like .pdf can have a significantly different file structure depending on what software was used to create it, the user's choices, and the software's own choices in deciding how to structure its output. PAWLs and OpenContracts attempt to solve this by sending every document through a processing pipeline that provides a uniform and consistent way of extracting and structuring text and layout information. Using the parsing engine of Grobid and the open source OCR engine Tesseract , every single document is re-OCRed (to produce a consistent output for the same inputs) and then the \"tokens\" (text surrounded on all sides by whitespace - typically a word) in the OCRed document are stored as JSONs with their page and positional information. In OpenContracts, we refer to this JSON layer that combines text and positional data as the \"PAWLs\" layer. We use the PAWLs layer to build the full text extract from the document as well and store this as the \"text layer\". Thus, in OpenContracts, every document has three files associated with it - the original pdf, a json file (the \"PAWLs layer\"), and a text file (the \"text layer\"). Because the text layer is built from the PAWLs layer, we can easily translate back and forth from text to positional information - e.g. given the start and end of a span of text the text layer, we can accurately say which PAWLs tokens the span includes, and, based on that, the x,y position of the span in the document. This lets us take the outputs of many NLP libraries which typically produce only start and stop ranges and layer them perfectly on top of the original pdf. With the PAWLs tokens as the source of truth, we can seamlessly transition from text only to layout-aware text.","title":"Data Layers"},{"location":"under-the-hood/#limitations","text":"OCR is not perfect. By only accepting pdf inputs and OCRing every document, we do ignore any text embedded in the pdf. To the extent that text was exported accurately from whatever tool was used to write the document, this introduces some potential loss of fidelity - e.g. if you've ever seen an OCR engine mistake an 'O' or a 0 or 'I' for a '1' or something like that. Typically, however, the instance of such errors is fairly small, and it's a price we have to pay for the power of being able to effortlessly layer NLP outputs that have no layout awareness on top of complex, visual layouts.","title":"Limitations"},{"location":"configuration/add-users/","text":"Adding More Users \u00b6 You can use the same User admin page described above to create new users. Alternatively, go back to the main admin page http://localhost:8000/admin and, under the User section, click the \"+Add\" button: Then, follow the on-screen instructions: When you're done, the username and password you provided can be used to login. OpenContracts is currently not built to allow users to self-register unless you use the Auth0 authentication. When managing users yourself, you'll need to add, remove and modify users via the admin panels.","title":"Add Users"},{"location":"configuration/add-users/#adding-more-users","text":"You can use the same User admin page described above to create new users. Alternatively, go back to the main admin page http://localhost:8000/admin and, under the User section, click the \"+Add\" button: Then, follow the on-screen instructions: When you're done, the username and password you provided can be used to login. OpenContracts is currently not built to allow users to self-register unless you use the Auth0 authentication. When managing users yourself, you'll need to add, remove and modify users via the admin panels.","title":"Adding More Users"},{"location":"configuration/choose-an-authentication-backend/","text":"Select Authentication System via Env Variables \u00b6 For authentication and authorization, you have two choices. 1. You can configure an Auth0 account and use Auth0 to authenticate users, in which case anyone who is permitted to authenticate via your auth0 setup can login and automatically get an account, 2. or, you can require a username and password for each user and our OpenContracts backend can provide user authentication and authorization. Using the latter option, there is no currently-supported sign-up method, you'll need to use the admin dashboard (See \"Adding Users\" section). Auth0 Auth Setup \u00b6 You need to configure three, separate applications on Auth0's platform: Configure the SPA as an application. You'll need the App Client ID. Configure the API. You'll need API Audience. Configure a M2M application to access the Auth0 Management API. This is used to fetch user details. You'll need the API_ID for the M2M application and the Client Secret for the M2M app. You'll also need your Auth0 tenant ID (assuming it's the same for all three applications, though you could, in theory, host them in different tenants). These directions are not comprehensive, so, if you're not familiar with Auth0, we recommend you disable Auth0 for the time being and use username and password. To enable and configure Auth0 Authentication, you'll need to set the following env variables in your .env file (the .django file in .envs/.production or .envs/.local , depending on your target environment). Our sample .envs only show these fields in the .production sample, but you could use them in the .local env file too: USE_AUTH0 - set to true to enable Auth0 AUTH0_CLIENT_ID - should be the client ID configured on Auth0 AUTH0_API_AUDIENCE - Configured API audience AUTH0_DOMAIN - domain of your configured Auth0 application AUTH0_M2M_MANAGEMENT_API_SECRET - secret for the auth0 Machine to Machine (M2M) API AUTH0_M2M_MANAGEMENT_API_ID - ID for Auth0 Machine to Machine (M2M) API AUTH0_M2M_MANAGEMENT_GRANT_TYPE - set to client_credentials Detailed Explanation of Auth0 Implementation \u00b6 To get Auth0 to work nicely with Graphene, we modified the graphql_jwt backend to support syncing remote user metadata with a local user similar to the default, django RemoteUserMiddleware . We're keeping the graphql_jwt graphene middleware in its entirety as it fetches the token and then passes it along to django authentication *backend. That django backend is what we're modifying to decode the jwt token against Auth0 settings and then check to see if local user exists, and, if not, create it. Here's the order of operations in the original Graphene backend provided by graphql_jwt: Backend's authenticate method is called from the graphene middleware via django ( from django.contrib.auth import authenticate ) token is retrieved via .utils get_credentials if token is not None, get_user_by_token in shortcuts module is called \"Payload\" is retrieved via utils.get_payload User is requested via utils.get_user_by_payload username is retrieved from payload via auth0_settings.JWT_PAYLOAD_GET_USERNAME_HANDLER user object is retrieved via auth0_settings.JWT_GET_USER_BY_NATURAL_KEY_HANDLER We modified a couple things: The decode method called in 3(a) needs to be modified to decode with Auth0 secrets and settings. get_user_by_payload needs to be modified in several ways: user object must use RemoteUserMiddleware logic and, if everything from auth0 decodes properly, check to see if user with e-mail exists and, if not, create it. Upon completion of this, try to sync user data with auth0. 2) return created or retrieved user object as original method did Django-Based Authentication Setup \u00b6 The only thing you need to do for this is toggle the two auth0-related environment variables: 1. For the backend environment, set USE_AUTH0=False in your environment (either via an environment variable file or directly in your environment via the console). 2. For the frontend environment, set REACT_APP_USE_AUTH0=false in your environment (either via an environment variable file or directly in your environment via the console). Note As noted elsewhere, users cannot sign up on their own . You need to log into the admin dashboard - e.g. http://localhost:8000/admin - and add users manually.","title":"Configure Authentication Backend"},{"location":"configuration/choose-an-authentication-backend/#select-authentication-system-via-env-variables","text":"For authentication and authorization, you have two choices. 1. You can configure an Auth0 account and use Auth0 to authenticate users, in which case anyone who is permitted to authenticate via your auth0 setup can login and automatically get an account, 2. or, you can require a username and password for each user and our OpenContracts backend can provide user authentication and authorization. Using the latter option, there is no currently-supported sign-up method, you'll need to use the admin dashboard (See \"Adding Users\" section).","title":"Select Authentication System via Env Variables"},{"location":"configuration/choose-an-authentication-backend/#auth0-auth-setup","text":"You need to configure three, separate applications on Auth0's platform: Configure the SPA as an application. You'll need the App Client ID. Configure the API. You'll need API Audience. Configure a M2M application to access the Auth0 Management API. This is used to fetch user details. You'll need the API_ID for the M2M application and the Client Secret for the M2M app. You'll also need your Auth0 tenant ID (assuming it's the same for all three applications, though you could, in theory, host them in different tenants). These directions are not comprehensive, so, if you're not familiar with Auth0, we recommend you disable Auth0 for the time being and use username and password. To enable and configure Auth0 Authentication, you'll need to set the following env variables in your .env file (the .django file in .envs/.production or .envs/.local , depending on your target environment). Our sample .envs only show these fields in the .production sample, but you could use them in the .local env file too: USE_AUTH0 - set to true to enable Auth0 AUTH0_CLIENT_ID - should be the client ID configured on Auth0 AUTH0_API_AUDIENCE - Configured API audience AUTH0_DOMAIN - domain of your configured Auth0 application AUTH0_M2M_MANAGEMENT_API_SECRET - secret for the auth0 Machine to Machine (M2M) API AUTH0_M2M_MANAGEMENT_API_ID - ID for Auth0 Machine to Machine (M2M) API AUTH0_M2M_MANAGEMENT_GRANT_TYPE - set to client_credentials","title":"Auth0 Auth Setup"},{"location":"configuration/choose-an-authentication-backend/#detailed-explanation-of-auth0-implementation","text":"To get Auth0 to work nicely with Graphene, we modified the graphql_jwt backend to support syncing remote user metadata with a local user similar to the default, django RemoteUserMiddleware . We're keeping the graphql_jwt graphene middleware in its entirety as it fetches the token and then passes it along to django authentication *backend. That django backend is what we're modifying to decode the jwt token against Auth0 settings and then check to see if local user exists, and, if not, create it. Here's the order of operations in the original Graphene backend provided by graphql_jwt: Backend's authenticate method is called from the graphene middleware via django ( from django.contrib.auth import authenticate ) token is retrieved via .utils get_credentials if token is not None, get_user_by_token in shortcuts module is called \"Payload\" is retrieved via utils.get_payload User is requested via utils.get_user_by_payload username is retrieved from payload via auth0_settings.JWT_PAYLOAD_GET_USERNAME_HANDLER user object is retrieved via auth0_settings.JWT_GET_USER_BY_NATURAL_KEY_HANDLER We modified a couple things: The decode method called in 3(a) needs to be modified to decode with Auth0 secrets and settings. get_user_by_payload needs to be modified in several ways: user object must use RemoteUserMiddleware logic and, if everything from auth0 decodes properly, check to see if user with e-mail exists and, if not, create it. Upon completion of this, try to sync user data with auth0. 2) return created or retrieved user object as original method did","title":"Detailed Explanation of Auth0 Implementation"},{"location":"configuration/choose-an-authentication-backend/#django-based-authentication-setup","text":"The only thing you need to do for this is toggle the two auth0-related environment variables: 1. For the backend environment, set USE_AUTH0=False in your environment (either via an environment variable file or directly in your environment via the console). 2. For the frontend environment, set REACT_APP_USE_AUTH0=false in your environment (either via an environment variable file or directly in your environment via the console). Note As noted elsewhere, users cannot sign up on their own . You need to log into the admin dashboard - e.g. http://localhost:8000/admin - and add users manually.","title":"Django-Based Authentication Setup"},{"location":"configuration/choose-and-configure-docker-stack/","text":"Deployment Options \u00b6 OpenContracts is designed to be deployed using docker-compose. You can run it locally or in a production environment. Follow the instructions below for a local environment if you just want to test it or you want to use it for yourself and don't intend to make the application available to other users via the Internet. Local Deployment \u00b6 Quick Start with Default Settings \u00b6 A \"local\" deployment is deployed on your personal computer and is not meant to be accessed over the Internet. If you don't need to configure anything, just follow the quick start guide above to get up and running with a local deployment without needing any further configuration. Setup .env Files \u00b6 Backend \u00b6 After cloning this repo to a machine of your choice, create a folder for your environment files in the repo root. You'll need ./.envs/.local/.django and ./.envs/.local/.postgres Use the samples in ./documentation/sample_env_files/local as guidance. NOTE, you'll need to replace the placeholder passwords and users where noted, but, otherwise, minimal config should be required. Frontend \u00b6 In the ./frontend folder, you also need to create a single .env file which holds your configurations for your login method as well as certain feature switches (e.g. turn off imports). We've included a sample using auth0 and another sample using django's auth backend. Local vs production deployments are essentially the same, but the root url of the backend will change from localhost to whereever you're hosting the application in production. Build the Stack \u00b6 Once your .env files are setup, build the stack using docker-compose: $ docker-compose -f local.yml build Then, run migrations (to setup the database): $ docker-compose -f local.yml run django python manage.py migrate Then, create a superuser account that can log in to the admin dashboard (in a local deployment this is available at http://localhost:8000/admin ) by typing this command and following the prompts: $ docker-compose -f local.yml run django python manage.py createsuperuser Finally, bring up the stack: $ docker-compose -f local.yml up You should now be able to access the OpenContracts frontend by visiting http://localhost:3000 . Production Environment \u00b6 The production environment is designed to be public-facing and exposed to the Internet, so there are quite a number more configurations required than a local deployment, particularly if you use an AWS S3 storage backend or the Auth0 authentication system. After cloning this repo to a machine of your choice, configure the production .env files as described above. You'll also need to configure your website url. This needs to be done in a few places. First, in opencontractserver/contrib/migrations , you'll fine a file called 0003_set_site_domain_and_name.py . BEFORE running any of your migrations, you should modify the domain and name defaults you'll fine in update_site_forward : def update_site_forward(apps, schema_editor): \"\"\"Set site domain and name.\"\"\" Site = apps.get_model(\"sites\", \"Site\") Site.objects.update_or_create( id=settings.SITE_ID, defaults={ \"domain\": \"opencontracts.opensource.legal\", \"name\": \"OpenContractServer\", }, ) and update_site_backward : def update_site_backward(apps, schema_editor): \"\"\"Revert site domain and name to default.\"\"\" Site = apps.get_model(\"sites\", \"Site\") Site.objects.update_or_create( id=settings.SITE_ID, defaults={\"domain\": \"example.com\", \"name\": \"example.com\"} ) Finally, don't forget to configure Treafik, the router in the docker-compose stack that exposes different containers to end-users depending on the route (url) received you need to update the Treafik file here . If you're using Auth0, see the Auth0 configuration section . If you're using AWS S3 for file storage, see the AWS configuration section. NOTE, the underlying django library that provides cloud storage, django-storages, can also work with other cloud providers such as Azure and GCP. See the django storages library docs for more info. $ docker-compose -f production.yml build Then, run migrations (to setup the database): $ docker-compose -f production.yml run django python manage.py migrate` Then, create a superuser account that can log in to the admin dashboard (in a production deployment this is available at the url set in your env file as the DJANGO_ADMIN_URL ) by typing this command and following the prompts: $ docker-compose -f production.yml run django python manage.py createsuperuser Finally, bring up the stack: $ docker-compose -f production.yml up You should now be able to access the OpenContracts frontend by visiting http://localhost:3000 . ENV File Configurations \u00b6 OpenContracts is configured via .env files. For a local deployment, these should go in .envs/.local . For production, use .envs/.production . Sample .envs for each deployment environment are provided in documentation/sample_env_files . The local configuration should let you deploy the application on your PC without requiring any specific configuration. The production configuration is meant to provide a web application and requires quite a bit more configuration and knowledge of web apps. Include Gremlin \u00b6 If you want to include a Gremlin analyzer, use local_deploy_with_gremlin.yml or production_deploy_with_gremlin.yml instead of local.yml or production.yml , respectively. All other parts of the tutorial are the same.","title":"Choose and Configure Docker Compose Stack"},{"location":"configuration/choose-and-configure-docker-stack/#deployment-options","text":"OpenContracts is designed to be deployed using docker-compose. You can run it locally or in a production environment. Follow the instructions below for a local environment if you just want to test it or you want to use it for yourself and don't intend to make the application available to other users via the Internet.","title":"Deployment Options"},{"location":"configuration/choose-and-configure-docker-stack/#local-deployment","text":"","title":"Local Deployment"},{"location":"configuration/choose-and-configure-docker-stack/#quick-start-with-default-settings","text":"A \"local\" deployment is deployed on your personal computer and is not meant to be accessed over the Internet. If you don't need to configure anything, just follow the quick start guide above to get up and running with a local deployment without needing any further configuration.","title":"Quick Start with Default Settings"},{"location":"configuration/choose-and-configure-docker-stack/#setup-env-files","text":"","title":"Setup .env Files"},{"location":"configuration/choose-and-configure-docker-stack/#backend","text":"After cloning this repo to a machine of your choice, create a folder for your environment files in the repo root. You'll need ./.envs/.local/.django and ./.envs/.local/.postgres Use the samples in ./documentation/sample_env_files/local as guidance. NOTE, you'll need to replace the placeholder passwords and users where noted, but, otherwise, minimal config should be required.","title":"Backend"},{"location":"configuration/choose-and-configure-docker-stack/#frontend","text":"In the ./frontend folder, you also need to create a single .env file which holds your configurations for your login method as well as certain feature switches (e.g. turn off imports). We've included a sample using auth0 and another sample using django's auth backend. Local vs production deployments are essentially the same, but the root url of the backend will change from localhost to whereever you're hosting the application in production.","title":"Frontend"},{"location":"configuration/choose-and-configure-docker-stack/#build-the-stack","text":"Once your .env files are setup, build the stack using docker-compose: $ docker-compose -f local.yml build Then, run migrations (to setup the database): $ docker-compose -f local.yml run django python manage.py migrate Then, create a superuser account that can log in to the admin dashboard (in a local deployment this is available at http://localhost:8000/admin ) by typing this command and following the prompts: $ docker-compose -f local.yml run django python manage.py createsuperuser Finally, bring up the stack: $ docker-compose -f local.yml up You should now be able to access the OpenContracts frontend by visiting http://localhost:3000 .","title":"Build the Stack"},{"location":"configuration/choose-and-configure-docker-stack/#production-environment","text":"The production environment is designed to be public-facing and exposed to the Internet, so there are quite a number more configurations required than a local deployment, particularly if you use an AWS S3 storage backend or the Auth0 authentication system. After cloning this repo to a machine of your choice, configure the production .env files as described above. You'll also need to configure your website url. This needs to be done in a few places. First, in opencontractserver/contrib/migrations , you'll fine a file called 0003_set_site_domain_and_name.py . BEFORE running any of your migrations, you should modify the domain and name defaults you'll fine in update_site_forward : def update_site_forward(apps, schema_editor): \"\"\"Set site domain and name.\"\"\" Site = apps.get_model(\"sites\", \"Site\") Site.objects.update_or_create( id=settings.SITE_ID, defaults={ \"domain\": \"opencontracts.opensource.legal\", \"name\": \"OpenContractServer\", }, ) and update_site_backward : def update_site_backward(apps, schema_editor): \"\"\"Revert site domain and name to default.\"\"\" Site = apps.get_model(\"sites\", \"Site\") Site.objects.update_or_create( id=settings.SITE_ID, defaults={\"domain\": \"example.com\", \"name\": \"example.com\"} ) Finally, don't forget to configure Treafik, the router in the docker-compose stack that exposes different containers to end-users depending on the route (url) received you need to update the Treafik file here . If you're using Auth0, see the Auth0 configuration section . If you're using AWS S3 for file storage, see the AWS configuration section. NOTE, the underlying django library that provides cloud storage, django-storages, can also work with other cloud providers such as Azure and GCP. See the django storages library docs for more info. $ docker-compose -f production.yml build Then, run migrations (to setup the database): $ docker-compose -f production.yml run django python manage.py migrate` Then, create a superuser account that can log in to the admin dashboard (in a production deployment this is available at the url set in your env file as the DJANGO_ADMIN_URL ) by typing this command and following the prompts: $ docker-compose -f production.yml run django python manage.py createsuperuser Finally, bring up the stack: $ docker-compose -f production.yml up You should now be able to access the OpenContracts frontend by visiting http://localhost:3000 .","title":"Production Environment"},{"location":"configuration/choose-and-configure-docker-stack/#env-file-configurations","text":"OpenContracts is configured via .env files. For a local deployment, these should go in .envs/.local . For production, use .envs/.production . Sample .envs for each deployment environment are provided in documentation/sample_env_files . The local configuration should let you deploy the application on your PC without requiring any specific configuration. The production configuration is meant to provide a web application and requires quite a bit more configuration and knowledge of web apps.","title":"ENV File Configurations"},{"location":"configuration/choose-and-configure-docker-stack/#include-gremlin","text":"If you want to include a Gremlin analyzer, use local_deploy_with_gremlin.yml or production_deploy_with_gremlin.yml instead of local.yml or production.yml , respectively. All other parts of the tutorial are the same.","title":"Include Gremlin"},{"location":"configuration/choose-storage-backend/","text":"Select and Setup Storage Backend \u00b6 You can use Amazon S3 as a file storage backend (if you set the env flag USE_AWS=True , more on that below), or you can use the local storage of the host machine via a Docker volume. AWS Storage Backend \u00b6 If you want to use AWS S3 to store files (primarily pdfs, but also exports, tokens and txt files), you will need an Amazon AWS account to setup S3. This README does not cover the AWS side of configuration, but there are a number of tutorials and guides to getting AWS configured to be used with a django project. Once you have an S3 bucket configured, you'll need to set the following env variables in your .env file (the .django file in .envs/.production or .envs/.local , depending on your target environment). Our sample .envs only show these fields in the .production samples, but you could use them in the .local env file too. Here the variables you need to set to enable AWS S3 storage: USE_AWS - set to true since you're using AWS, otherwise the backend will use a docker volume for storage. DJANGO_AWS_ACCESS_KEY_ID - the access key ID created by AWS when you set up your IAM user (see tutorials above). DJANGO_AWS_SECRET_ACCESS_KEY - the secret access key created by AWS when you set up your IAM user (see tutorials above) DJANGO_AWS_STORAGE_BUCKET_NAME - the name of the AWS bucket you created to hold the files. DJANGO_AWS_S3_REGION_NAME - the region of the AWS bucket you configured. Django Storage Backend \u00b6 Setting USE_AWS=false will use the disk space in the django container. When using the local docker compose stack, the celery workers and django containers share the same disk, so this works fine. Our production configuration would not work properly with USE_AWS=false , however, as each container has its own disk.","title":"Configure Storage Backend"},{"location":"configuration/choose-storage-backend/#select-and-setup-storage-backend","text":"You can use Amazon S3 as a file storage backend (if you set the env flag USE_AWS=True , more on that below), or you can use the local storage of the host machine via a Docker volume.","title":"Select and Setup Storage Backend"},{"location":"configuration/choose-storage-backend/#aws-storage-backend","text":"If you want to use AWS S3 to store files (primarily pdfs, but also exports, tokens and txt files), you will need an Amazon AWS account to setup S3. This README does not cover the AWS side of configuration, but there are a number of tutorials and guides to getting AWS configured to be used with a django project. Once you have an S3 bucket configured, you'll need to set the following env variables in your .env file (the .django file in .envs/.production or .envs/.local , depending on your target environment). Our sample .envs only show these fields in the .production samples, but you could use them in the .local env file too. Here the variables you need to set to enable AWS S3 storage: USE_AWS - set to true since you're using AWS, otherwise the backend will use a docker volume for storage. DJANGO_AWS_ACCESS_KEY_ID - the access key ID created by AWS when you set up your IAM user (see tutorials above). DJANGO_AWS_SECRET_ACCESS_KEY - the secret access key created by AWS when you set up your IAM user (see tutorials above) DJANGO_AWS_STORAGE_BUCKET_NAME - the name of the AWS bucket you created to hold the files. DJANGO_AWS_S3_REGION_NAME - the region of the AWS bucket you configured.","title":"AWS Storage Backend"},{"location":"configuration/choose-storage-backend/#django-storage-backend","text":"Setting USE_AWS=false will use the disk space in the django container. When using the local docker compose stack, the celery workers and django containers share the same disk, so this works fine. Our production configuration would not work properly with USE_AWS=false , however, as each container has its own disk.","title":"Django Storage Backend"},{"location":"configuration/configure-admin-users/","text":"Gremlin Admin Dashboard \u00b6 Gremlin's backend is built on Django, which has its own powerful admin dashboard. This dashboard is not meant for end-users and should only be used by admins. You can access the admin dashboard by going to the /admin page - e,g, opencontracts.opensource.legal/admin or http://localhost:8000/admin . For the most part, you shouldn't need to use the admin dashboard and should only go in here if you're experience errors or unexpected behavior and want to look at the detailed contents of the database to see if it sheds any light on what's happening with a give corpus, document, etc. By default, Gremlin creates an admin user for you. If you don't specify the username and password in your environment on first boot, it'll use system defaults. You can customize the default username and password via environment variables or after the system boots using the admin dash. Configure Username and Password Prior to First Deployment \u00b6 If the variable DJANGO_SUPERUSER_USERNAME is set, that will be the default admin user created on startup (the first time your run docker-compose -f local.yml up ). The repo ships with a default superuser username of admin . The default password is set using the DJANGO_SUPERUSER_PASSWORD variable. The environment files for local deployments ( but not production ) include a default password of Openc0ntracts_def@ult . You should change this in the environment file before the first start OR, follow the instructions below to change it after the first start. If you modify these environment variables in the environment file BEFORE running the docker-compose up command for the first time, your initial superuser will have the username, email and/or password you specify. If you don't modify the defaults, you can change them after you have created them via the admin dashboard (see below). After First Deployment via Admin Dashboard \u00b6 Once the default superuser has been created, you'll need to use the admin dashboard to modify it. To manage users, including changing the password, you'll need to access the backend admin dashboard. OpenContracts is built on Django, which ships with Django Admin , a tool to manage low-level object data and users. It doesn't provide the rich, document focused UI/UX our frontend does, but it does let you edit and delete objects created on the frontend if, for any reason, you are unable to fix something done by a frontend user (e.g. a corrupt file is uploaded and cannot be parsed or rendered properly on the frontend). To update your users, first login to the admin panel: Then, in the lefthand navbar, find the entry for \"Users\" and click on it Then, you'll see a list of all users for this instance. You should see your admin user and an \"Anonymous\" user. The Anonymous user is required for public browsing of objcets with their is_public field set to True. The Anonymous user cannot see other objects. Click on the admin user to bring up the detailed user view: Now you can click the \"WHAT AM I CALLED\" button to bring up a dialog to change the user password.","title":"Configure Admin Users"},{"location":"configuration/configure-admin-users/#gremlin-admin-dashboard","text":"Gremlin's backend is built on Django, which has its own powerful admin dashboard. This dashboard is not meant for end-users and should only be used by admins. You can access the admin dashboard by going to the /admin page - e,g, opencontracts.opensource.legal/admin or http://localhost:8000/admin . For the most part, you shouldn't need to use the admin dashboard and should only go in here if you're experience errors or unexpected behavior and want to look at the detailed contents of the database to see if it sheds any light on what's happening with a give corpus, document, etc. By default, Gremlin creates an admin user for you. If you don't specify the username and password in your environment on first boot, it'll use system defaults. You can customize the default username and password via environment variables or after the system boots using the admin dash.","title":"Gremlin Admin Dashboard"},{"location":"configuration/configure-admin-users/#configure-username-and-password-prior-to-first-deployment","text":"If the variable DJANGO_SUPERUSER_USERNAME is set, that will be the default admin user created on startup (the first time your run docker-compose -f local.yml up ). The repo ships with a default superuser username of admin . The default password is set using the DJANGO_SUPERUSER_PASSWORD variable. The environment files for local deployments ( but not production ) include a default password of Openc0ntracts_def@ult . You should change this in the environment file before the first start OR, follow the instructions below to change it after the first start. If you modify these environment variables in the environment file BEFORE running the docker-compose up command for the first time, your initial superuser will have the username, email and/or password you specify. If you don't modify the defaults, you can change them after you have created them via the admin dashboard (see below).","title":"Configure Username and Password Prior to First Deployment"},{"location":"configuration/configure-admin-users/#after-first-deployment-via-admin-dashboard","text":"Once the default superuser has been created, you'll need to use the admin dashboard to modify it. To manage users, including changing the password, you'll need to access the backend admin dashboard. OpenContracts is built on Django, which ships with Django Admin , a tool to manage low-level object data and users. It doesn't provide the rich, document focused UI/UX our frontend does, but it does let you edit and delete objects created on the frontend if, for any reason, you are unable to fix something done by a frontend user (e.g. a corrupt file is uploaded and cannot be parsed or rendered properly on the frontend). To update your users, first login to the admin panel: Then, in the lefthand navbar, find the entry for \"Users\" and click on it Then, you'll see a list of all users for this instance. You should see your admin user and an \"Anonymous\" user. The Anonymous user is required for public browsing of objcets with their is_public field set to True. The Anonymous user cannot see other objects. Click on the admin user to bring up the detailed user view: Now you can click the \"WHAT AM I CALLED\" button to bring up a dialog to change the user password.","title":"After First Deployment via Admin Dashboard"},{"location":"configuration/configure-gremlin/","text":"Gremlin is a separate project by OpenSource Legal to provide a standard API to access NLP capabilities. This lets us wrap multiple NLP engines / techniques in the same API which lets us build tools that can readily consume the outputs of very different NLP libraries (etc. a Transformers-based model like BERT, and tools like SPACY and LexNLP can be deployed on Gremlin and the outputs from all three can readily be rendered in OpenContracts). OpenContracts is designed to work with Gremlin out-of-the-box. We have a sample compose yaml file showing how to do this on a local machine local_deploy_with_gremlin.yaml and as a web-facing application production_deploy_with_gremlin.yaml . When you add a new Gremlin Engine to the database, OpenContracs will automatically query it for its installed analyzers and labels. These will then be available within OpenContracts, and you can use an analyzer to analyze any OpenContracts corpus. While we have plans to automatically \"install\" the default Gremlin on first boot, currently you must manually go into the OpenContracts admin dash and add the Gremlin. Thankfully, this is an easy process: In your environment file, make sure you set CALLBACK_ROOT_URL_FOR_ANALYZER For local deploy, use CALLBACK_ROOT_URL_FOR_ANALYZER=http://localhost:8000 For production deploy, use http://django:5000 . Why the change? Well, in our local docker compose stack, the host the localhost and the django development server runs on port 8000. In production, we want Gremlin to communicate with the OpenContracts container (\"django\") via its hostname on the docker compose stack's network. The production OpenContracts container also uses gunicorn on port 5000 instead of the development server on port 8000, so the port changes too. Go to the admin page: Click \"Add+\" in the Gremlin row to bring up the Add Gremlin Engine form. You just need to set the creator Url fields (the url for our default config is http://gremlinengine:5000 ). If, for some reason, you don't want the analyzer to be visible to any unauthenticated user, unselect the is_public box : This will automatically kick off an install process that runs in the background. When it's complete, you'll see the \"Install Completed\" Field change. It should take a second or two. At the moment, we don't handle errors in this process, so, if it doesn't complete successfully in 30 seconds, there is probably a misconfiguration somewhere. We plan to improve our error handling for these backend installation processes. Note, in our example implementations, Gremlin is NOT encrypted or API Key secured to outside traffic. It's not exposed to outside traffic either per our docker compose config, so this shouldn't be a major concern. If you do expose the container to the host via your Docker Compose file, you should ensure you run the traffic through Treafik and setup API Key authentication.","title":"Configure Gremlin Analyzer"},{"location":"development/documentation/","text":"Documentation Stack \u00b6 We're using mkdocs to render our markdown into pretty, bite-sized pieces. The markdown lives in /docs in our repo. If you want to work on the docs you'll need to install the requirements in /requirements/docs.txt . To have a live server while working on them, type: mkdocs serve Building Docs \u00b6 To build a html website from your markdown that can be uploaded to a webhost (or a GitHub Page), just type: mkdocs build Deploying to GH Page \u00b6 mkdocs makes it super easy to deploy your docs to a GitHub page. Just run: mkdocs gh-deploy","title":"Documentation"},{"location":"development/documentation/#documentation-stack","text":"We're using mkdocs to render our markdown into pretty, bite-sized pieces. The markdown lives in /docs in our repo. If you want to work on the docs you'll need to install the requirements in /requirements/docs.txt . To have a live server while working on them, type: mkdocs serve","title":"Documentation Stack"},{"location":"development/documentation/#building-docs","text":"To build a html website from your markdown that can be uploaded to a webhost (or a GitHub Page), just type: mkdocs build","title":"Building Docs"},{"location":"development/documentation/#deploying-to-gh-page","text":"mkdocs makes it super easy to deploy your docs to a GitHub page. Just run: mkdocs gh-deploy","title":"Deploying to GH Page"},{"location":"development/environment/","text":"We use Black and Flake8 for Python Code Styling. These are run via pre-commit before all commits. If you want to develop extensions or code based on OpenContracts, you'll need to setup pre-commit. First, make sure the requirements in ./requirements/local.txt are installed in your local environment. Then, install pre-commit into your local git repo. From the root of the repo, run: $ pre-commit install If you want to run pre-commit manually on all the code in the repo, use this command: $ pre-commit run --all-files When you commit changes to your repo or our repo as a PR, pre-commit will run and ensure your code follows our style guide and passes linting.","title":"Dev Environment"},{"location":"development/frontend-notes/","text":"Responsive Layout \u00b6 The application was primarily designed to be viewed around 1080p. We've built in some quick and dirty (honestly, hacks) to display a usable layout at other resolutions. A more thorough redesign / refactor is in order, again if there's sufficient interest. What's available now should handle a lot of situations ok. If you find performance / layout is not looking great at your given resolution, try to use a desktop browser at a 1080p resolution. No Test Suite \u00b6 As of our initial release, the test suite only tests the backend (and coverage is admittedly not as robust as we'd like). We'd like to add tests for the frontend, though this is a fairly large undertaking. We welcome any contributions on this front!","title":"Frontend Notes"},{"location":"development/frontend-notes/#responsive-layout","text":"The application was primarily designed to be viewed around 1080p. We've built in some quick and dirty (honestly, hacks) to display a usable layout at other resolutions. A more thorough redesign / refactor is in order, again if there's sufficient interest. What's available now should handle a lot of situations ok. If you find performance / layout is not looking great at your given resolution, try to use a desktop browser at a 1080p resolution.","title":"Responsive Layout"},{"location":"development/frontend-notes/#no-test-suite","text":"As of our initial release, the test suite only tests the backend (and coverage is admittedly not as robust as we'd like). We'd like to add tests for the frontend, though this is a fairly large undertaking. We welcome any contributions on this front!","title":"No Test Suite"},{"location":"development/test-suite/","text":"Our test suite is a bit sparse, but we're working to improve coverage on the backend. Frontend tests will likely take longer to implement. Our existing tests do test imports and a number of the utility functions for manipulating annotations. These tests are integrated in our GitHub actions. NOTE, use Python 3.10 or above as pydantic and certain pre-3.10 type annotations do not play well. using from __future__ import annotations doesn't always solve the problem, and upgrading to Python 3.10 was a lot easier than trying to figure out why the from __future__ didn't behave as expected To run the tests, check your test coverage, and generate an HTML coverage report: $ docker-compose -f local.yml run django coverage run -m pytest $ docker-compose -f local.yml run django coverage html $ open htmlcov/index.html To run a specific test (e.g. test_analyzers): $ sudo docker-compose -f local.yml run django python manage.py test opencontractserver.tests.test_analyzers --noinput","title":"Test Suite"},{"location":"walkthrough/key-concepts/","text":"Data Types \u00b6 Text annotation data is divided into several concepts: Corpuses (or collections of documents). One document can be in multiple corpuses. Documents . Currently, these are PDFs ONLY. Annotations . These are either document-level annotations (the document type), text-level annotations (highlighted text), or relationships (which apply a label between two annotations). Relationships are currently not well-supported and may be buggy. Analyses . These groups of read-only annotations added by a Gremlin analyzer (see more on that below). Permissioning \u00b6 OpenContracts is built on top of the powerful permissioning framework for Django called django-guardian . Each GraphQL request can add a field to annotate the object-level permissions the current user has for a given object, and the frontend relies on this to determine whether to make some objects and pages read-only and whether certain features should be exposed to a given user. The capability of sharing objects with specific users is built in, but is not enabled from the frontend at the moment. Allowing such widespread sharing and user lookups could be a security hole and could also unduly tax the system. We'd like to test these capabilities more fully before letting users used them. GraphQL \u00b6 Mutations and Queries \u00b6 OpenContracts uses Graphene and GraphQL to serve data to its frontend. You can access the Graphiql playground by going to your OpenContracts root url /graphql - e.g. https://opencontracts.opensource.legal/graphql . Anonymous users have access to any public data. To authenticate and access your own data, you either need to use the login mutation to create a JWT token or login to the admin dashboard to get a Django session and auth cookie that will automatically authenticate your requests to the GraphQL endpoint. If you're not familiar with GraphQL , it's a very powerful way to expose your backend to the user and/or frontend clients to permit the construction of specific queries with specific data shapes. As an example, here's a request to get public corpuses and the annotated text and labels in them: Graphiql comes with a built-in documentation browser. Just click \"Docs\" in the top-right of the screen to start browsing. Typically, mutations change things on the server. Queries merely request copies of data from the server. We've tried to make our schema fairly self-explanatory, but we do plan to add more descriptions and guidance to our API docs. GraphQL-only features \u00b6 Some of our features are currently not accessible via the frontend. Sharing analyses and corpuses to the public, for example, can only be achieved via makeCorpusPublic and makeAnalysisPublic mutations, and only admins have this power at the moment. For our current release, we've done this to prevent large numbers of public corpuses being shared to cut down on server usage. We'd like to make a fully free and open, collaborative platform with more features to share anonymously, but this will require additional effort and compute power.","title":"Key-Concepts"},{"location":"walkthrough/key-concepts/#data-types","text":"Text annotation data is divided into several concepts: Corpuses (or collections of documents). One document can be in multiple corpuses. Documents . Currently, these are PDFs ONLY. Annotations . These are either document-level annotations (the document type), text-level annotations (highlighted text), or relationships (which apply a label between two annotations). Relationships are currently not well-supported and may be buggy. Analyses . These groups of read-only annotations added by a Gremlin analyzer (see more on that below).","title":"Data Types"},{"location":"walkthrough/key-concepts/#permissioning","text":"OpenContracts is built on top of the powerful permissioning framework for Django called django-guardian . Each GraphQL request can add a field to annotate the object-level permissions the current user has for a given object, and the frontend relies on this to determine whether to make some objects and pages read-only and whether certain features should be exposed to a given user. The capability of sharing objects with specific users is built in, but is not enabled from the frontend at the moment. Allowing such widespread sharing and user lookups could be a security hole and could also unduly tax the system. We'd like to test these capabilities more fully before letting users used them.","title":"Permissioning"},{"location":"walkthrough/key-concepts/#graphql","text":"","title":"GraphQL"},{"location":"walkthrough/key-concepts/#mutations-and-queries","text":"OpenContracts uses Graphene and GraphQL to serve data to its frontend. You can access the Graphiql playground by going to your OpenContracts root url /graphql - e.g. https://opencontracts.opensource.legal/graphql . Anonymous users have access to any public data. To authenticate and access your own data, you either need to use the login mutation to create a JWT token or login to the admin dashboard to get a Django session and auth cookie that will automatically authenticate your requests to the GraphQL endpoint. If you're not familiar with GraphQL , it's a very powerful way to expose your backend to the user and/or frontend clients to permit the construction of specific queries with specific data shapes. As an example, here's a request to get public corpuses and the annotated text and labels in them: Graphiql comes with a built-in documentation browser. Just click \"Docs\" in the top-right of the screen to start browsing. Typically, mutations change things on the server. Queries merely request copies of data from the server. We've tried to make our schema fairly self-explanatory, but we do plan to add more descriptions and guidance to our API docs.","title":"Mutations and Queries"},{"location":"walkthrough/key-concepts/#graphql-only-features","text":"Some of our features are currently not accessible via the frontend. Sharing analyses and corpuses to the public, for example, can only be achieved via makeCorpusPublic and makeAnalysisPublic mutations, and only admins have this power at the moment. For our current release, we've done this to prevent large numbers of public corpuses being shared to cut down on server usage. We'd like to make a fully free and open, collaborative platform with more features to share anonymously, but this will require additional effort and compute power.","title":"GraphQL-only features"},{"location":"walkthrough/step-1-add-documents/","text":"In order to do anything, you need to add some documents to Gremlin. Go to the Documents tab \u00b6 Click on the \"Documents\" entry in the menu to bring up a view of all documents you have read and/or write access to: Open the Action Menu \u00b6 Now, click on the \"Action\" dropdown to open the Action menu for available actions and click \"Import\": This will bring up a dialog to load documents: Select Documents to Upload \u00b6 Open Contracts works with PDFs only (as this helps us have a single file type with predictable data structures, formats, etc.). In the future, we'll add functionality to convert other files to PDF, but, for now, please use PDFs. It doesn't matter if they are OCRed or not as OpenContracts performs its own OCR on every PDF anyway to ensure consistent OCR quality and outputs. Once you've added documents for upload, you'll see a list of documents: Click on a document to change the description or title: Upload Your Documents \u00b6 Click upload to upload the documents to OpenContracts. Note Once the documents are uploaded, they are automatically processed with Tesseract amd PAWLs to create a layer of tokens - each one representing a word / symbol in the PDF an its X,Y coordinates on the page. This is what powers OpenContracts annotator and allows us to create both layout-aware and text-only annotations. While the PAWLs processing script is running, the document you uploaded will not be available for viewing and cannot be added to a corpus. You'll see a loading bar on the document until the pre-processing is complete. This is only one once and can take a long time (a couple of minutes to a max of 10) depending on the document length, quality, etc.","title":"Step 1 - Add Documents"},{"location":"walkthrough/step-1-add-documents/#go-to-the-documents-tab","text":"Click on the \"Documents\" entry in the menu to bring up a view of all documents you have read and/or write access to:","title":"Go to the Documents tab"},{"location":"walkthrough/step-1-add-documents/#open-the-action-menu","text":"Now, click on the \"Action\" dropdown to open the Action menu for available actions and click \"Import\": This will bring up a dialog to load documents:","title":"Open the Action Menu"},{"location":"walkthrough/step-1-add-documents/#select-documents-to-upload","text":"Open Contracts works with PDFs only (as this helps us have a single file type with predictable data structures, formats, etc.). In the future, we'll add functionality to convert other files to PDF, but, for now, please use PDFs. It doesn't matter if they are OCRed or not as OpenContracts performs its own OCR on every PDF anyway to ensure consistent OCR quality and outputs. Once you've added documents for upload, you'll see a list of documents: Click on a document to change the description or title:","title":"Select Documents to Upload"},{"location":"walkthrough/step-1-add-documents/#upload-your-documents","text":"Click upload to upload the documents to OpenContracts. Note Once the documents are uploaded, they are automatically processed with Tesseract amd PAWLs to create a layer of tokens - each one representing a word / symbol in the PDF an its X,Y coordinates on the page. This is what powers OpenContracts annotator and allows us to create both layout-aware and text-only annotations. While the PAWLs processing script is running, the document you uploaded will not be available for viewing and cannot be added to a corpus. You'll see a loading bar on the document until the pre-processing is complete. This is only one once and can take a long time (a couple of minutes to a max of 10) depending on the document length, quality, etc.","title":"Upload Your Documents"},{"location":"walkthrough/step-2-create-labelset/","text":"Why Labelsets? \u00b6 Before you can add labels, you need to decide what you want to label. A labelset should reflect the taxonomy or concepts you want to associate with text in your document. This can be solely for the purpose of human review and retrieval, but we imagine many of you want to use it to train machine learning models. At the moment, there's no way to create a label in a corpus without creating a labelset and creating a label for the labelset (though we'd like to add that and welcome contributions). Create Text Labels \u00b6 Let's say we want to add some labels for \"Parties\", \"Termination Clause\", and \"Effective Date\". To do that, let's first create a LabelSet to hold the labels. Go to the labelset view and click the action button to bring up the action menu: Clicking on the \"Create Label Set\" item will bring up a modal to let you create labels: Now click on the new label set to edit the labels: A modal comes up that lets you edit three types of labels: Text Labels - are meant to label spans of text (\"highlights\") Relationship Labels - this feature is still under development, but it labels relationships bewteen text label (e.g. one labelled party is the \"Parent Company\" of another). Doc Type Labels - are meant to label what category the document belongs in - e.g. a \"Stock Purchase Agreement\" or an \"NDA\" Click the \"Text Labels\" tab to bring up a view of current labels for text annotations and an action button that lets you create new ones. There should be no labels when you first open this view\" Click the action button and then the \"Create Text Label\" dropdown item: You'll see a new, blank label in the list of text labels: Click the edit icon on the label to edit the label title, description, color and/or icon. To edit the icon or highlight color, hover over or click the giant tag icon on the left side of the label: Hit save to commit the changes to the database. Repeat for the other labels - \"Parties\", \"Termination Clause\", and \"Effective Date\": Create Document-Type Labels \u00b6 In addition to labelling specific parts of a document, you may want to tag a document itself as a certain type of document or addressing a certain subject. In this example, let's say we want to label some documents as \"contracts\" and others as \"not contracts\". Let's also create two example document type labels. Click the \"Doc Type Labels\" tab: As before, click the action button and the \"Create Document Type Label\" item to create a blank document type label: Repeat to create two doc type labels - \"Contract\" and \"Not Contract\": Hit \"Close\" to close the editor.","title":"Step 2 - Create Labelset"},{"location":"walkthrough/step-2-create-labelset/#why-labelsets","text":"Before you can add labels, you need to decide what you want to label. A labelset should reflect the taxonomy or concepts you want to associate with text in your document. This can be solely for the purpose of human review and retrieval, but we imagine many of you want to use it to train machine learning models. At the moment, there's no way to create a label in a corpus without creating a labelset and creating a label for the labelset (though we'd like to add that and welcome contributions).","title":"Why Labelsets?"},{"location":"walkthrough/step-2-create-labelset/#create-text-labels","text":"Let's say we want to add some labels for \"Parties\", \"Termination Clause\", and \"Effective Date\". To do that, let's first create a LabelSet to hold the labels. Go to the labelset view and click the action button to bring up the action menu: Clicking on the \"Create Label Set\" item will bring up a modal to let you create labels: Now click on the new label set to edit the labels: A modal comes up that lets you edit three types of labels: Text Labels - are meant to label spans of text (\"highlights\") Relationship Labels - this feature is still under development, but it labels relationships bewteen text label (e.g. one labelled party is the \"Parent Company\" of another). Doc Type Labels - are meant to label what category the document belongs in - e.g. a \"Stock Purchase Agreement\" or an \"NDA\" Click the \"Text Labels\" tab to bring up a view of current labels for text annotations and an action button that lets you create new ones. There should be no labels when you first open this view\" Click the action button and then the \"Create Text Label\" dropdown item: You'll see a new, blank label in the list of text labels: Click the edit icon on the label to edit the label title, description, color and/or icon. To edit the icon or highlight color, hover over or click the giant tag icon on the left side of the label: Hit save to commit the changes to the database. Repeat for the other labels - \"Parties\", \"Termination Clause\", and \"Effective Date\":","title":"Create Text Labels"},{"location":"walkthrough/step-2-create-labelset/#create-document-type-labels","text":"In addition to labelling specific parts of a document, you may want to tag a document itself as a certain type of document or addressing a certain subject. In this example, let's say we want to label some documents as \"contracts\" and others as \"not contracts\". Let's also create two example document type labels. Click the \"Doc Type Labels\" tab: As before, click the action button and the \"Create Document Type Label\" item to create a blank document type label: Repeat to create two doc type labels - \"Contract\" and \"Not Contract\": Hit \"Close\" to close the editor.","title":"Create Document-Type Labels"},{"location":"walkthrough/step-3-create-a-corpus/","text":"Purpose of the Corpus \u00b6 A \"Corpus\" is a collection of documents that can be annotated by hand or automatically by a \"Gremlin\" analyzer. In order to create a Corpus, you first need to create a Corpus and then add documents to it. Go to the Corpus Page \u00b6 First, login if you're not already logged in. Then, go the \"Corpus\" tab and click the \"Action\" dropdown to bring up the action menu: Click \"Create Corpus\" to bring up the Create Corpus dialog. If you've already created a labelset or have a pre-existing one, you can select it, otherwise you'll need to create and add one later: Assuming you created the labelset you want to use, when you click on the dropdown in the \"Label Set\" section, you should see your new labelset. Click on it to select it: You will now be able to open the corpus again, open documents in the corpus and start labelling. Add Documents to Corpus \u00b6 Once you have a corpus, go back to the document page to select documents to add. You can do this in one of two ways. Right-click on a document to show a context menu: Or, SHIFT + click on the documents you want to select in order to select multiple documents at once. A green checkmark will appear on selected documents. When you're done, click the \"Action\" A dialog will pop up asking you to select a corpus to add the documents to. Select the desired corpus and hit ok. You'll get a confirmation dialog. Hit OK. When you click on the Corpus you just added the documents to, you'll get a tabbed view of all of the documents, annotations and analyses for that Corpus. At this stage, you should see your documents: Congrats! You've created a corpus to hold annotations or perform an analysis! In order to start labelling it yourself, you need to create and then select a LabelSet, however. You do not need to do this to run an analyzer, however. Note : If you have an OpenContracts export file and proper permissions, you can also import a corpus, documents, annotations, and labels. This is disabled on our demo instance, however, to but down on server load and reduce opportunities to upload potentially malicious files. See the \"Advanced\" section for more details.","title":"Step 3 - Create Corpus"},{"location":"walkthrough/step-3-create-a-corpus/#purpose-of-the-corpus","text":"A \"Corpus\" is a collection of documents that can be annotated by hand or automatically by a \"Gremlin\" analyzer. In order to create a Corpus, you first need to create a Corpus and then add documents to it.","title":"Purpose of the Corpus"},{"location":"walkthrough/step-3-create-a-corpus/#go-to-the-corpus-page","text":"First, login if you're not already logged in. Then, go the \"Corpus\" tab and click the \"Action\" dropdown to bring up the action menu: Click \"Create Corpus\" to bring up the Create Corpus dialog. If you've already created a labelset or have a pre-existing one, you can select it, otherwise you'll need to create and add one later: Assuming you created the labelset you want to use, when you click on the dropdown in the \"Label Set\" section, you should see your new labelset. Click on it to select it: You will now be able to open the corpus again, open documents in the corpus and start labelling.","title":"Go to the Corpus Page"},{"location":"walkthrough/step-3-create-a-corpus/#add-documents-to-corpus","text":"Once you have a corpus, go back to the document page to select documents to add. You can do this in one of two ways. Right-click on a document to show a context menu: Or, SHIFT + click on the documents you want to select in order to select multiple documents at once. A green checkmark will appear on selected documents. When you're done, click the \"Action\" A dialog will pop up asking you to select a corpus to add the documents to. Select the desired corpus and hit ok. You'll get a confirmation dialog. Hit OK. When you click on the Corpus you just added the documents to, you'll get a tabbed view of all of the documents, annotations and analyses for that Corpus. At this stage, you should see your documents: Congrats! You've created a corpus to hold annotations or perform an analysis! In order to start labelling it yourself, you need to create and then select a LabelSet, however. You do not need to do this to run an analyzer, however. Note : If you have an OpenContracts export file and proper permissions, you can also import a corpus, documents, annotations, and labels. This is disabled on our demo instance, however, to but down on server load and reduce opportunities to upload potentially malicious files. See the \"Advanced\" section for more details.","title":"Add Documents to Corpus"},{"location":"walkthrough/step-4-create-text-annotations/","text":"To view or edit annotations, you need to open a corpus and then open a document in the Corpus. Go to your Corpuses page and click on the corpus you just created: This will open up the document view again. Click on one of the documents to bring up the annotator: To select the label to apply, Click the vertical ellipses in the \"Text Label to Apply Widget\". This will bring up an interface that lets you search your labelset and select a label: Select the \"Effective Date\" label, for example, to label the Effective Date: Now, in the document, click and drag a box around the language that corresponds to your select label: When you've selected the correct text, release the mouse. You'll see a confirmtion when your annotation is created (you'll also see the annotation in the sidebar to the left): If you want to delete the annotation, you can click on the trash icon in the corresponding annotation card in the sidebar, or, when you hover over the annotation on the page, you'll see a trash icon in the label bar of the annotation. You can click this to delete the annotation too. If your desired annotated text is non-contiguous , you can hold down the SHIFT key while selecting blocks of text to combine them into a single annotation. While holding SHIFT, releasing the mouse will not create the annotation in the database, it will just allow you to move to a new area. One situation you might want to do this is where what you want to highlight is on different lines but is just a small part of the surrounding paragraph (such as this example, where Effective Date spans two lines): Or you might want to select multiple snippets of text in a larger block of text, such as where you have multiple parties you want to combine into a single annotation:","title":"Step 4 - Create Some Annotations"},{"location":"walkthrough/step-5-create-doc-type-annotations/","text":"If you want to label the type of document instead of the text inside it, use the controls in the \"Doc Type\" widget on the bottom right of the Annotator. Hover over it and a green plus button should appear: Click the \"+\" button to bring up a dialog that lets you search and select document type labels (remember, we created these earlier in the tutorial): Click \"Add Label\" to actually apply the label, and you'll now see that label displayed in the \"Doc Type\" widget in the annotator: As before, you can click the trash can to delete the label.","title":"Step 5 - Create Some Document Annotations"},{"location":"walkthrough/step-6-search-and-filter-by-annotations/","text":"Back in the Corpus view, you can see in the document view the document type label you just added: You can click on the filter dropdown above to filter the documents to only those with a certain doc type label: With the corpus opened, click on the \"Annotations\" tab instead of the \"Documents\" tab to get a summary of all the current annotations in the Corpus: Click on an annotation card to automatically load the document it's in and jump right to the page containing the annotation:","title":"Step 6 - Search and Filter By Annotations"},{"location":"walkthrough/advanced/configure-annotation-view/","text":"Annotations are composed of tokens (basically text in a line surrounded by whitespace). The tokens have a highlight. OpenContracts also has a \"BoundingBox\" around the tokens which is the smallest rectangle that can cover all of the tokens in an Annotation. In the Annotator view, you'll see a purple-colored \"eye\" icon in the top left of the annotation list in the sidebar. Click the icon to bring up a series of configurations for how annotations are displayed: There are three different settings that can be combined to significantly change how you see the annotations: 1. Show only selected - You will only see the annotation selected, either by clicking on it in the sidebar or when you clicked into an annotation from the Corpus view. All other annotations will be completely hidden. 2. Show bounding boxes - If you unselect this, only the tokens will be visible. This is recommended where you large numbers of overlapping annotations or annotations that are sparse - e.g. a few words scattered throughout a paragraph. In either of these cases, the bounding boxes can cover other bounding boxes and this can be confusing. Where you have too many overlapping bounding boxes, it's easier to hide them and just look at the tokens. 3. Label Display Behavior - has three options: Always Show - Always show the label for an annotation when it's displayed (remember, you can choose to only display selected annotations). Always Hide - Never show the label for an annotation, regardless of its visiblity. Show on Hover - If an annotation is visible, when you hover over it, you'll see the label.","title":"Configure How Annotations Are Displayed"},{"location":"walkthrough/advanced/export-import-corpuses/","text":"Exports \u00b6 OpenContracts support both exporting and importing corpuses. This functionality is disabled on the public demo as it can be bandwidth intensive. If you want to experiment with these features on your own, you'll see the export action when you right-click on a corpus: You can access your exports from the user dropdown menu in the top right corner of the screen. Once your export is complete, you should be able to download a zip containing all the documents, their PAWLs layers, and the corpus data you created - including all annotations. Imports \u00b6 If you've enabled corpus imports (see the frontend env file for the boolean toggle to do this - it's REACT_APP_ALLOW_IMPORTS ), you'll see an import action when you click the action button on the corpus page.","title":"Import and Export Corpuses"},{"location":"walkthrough/advanced/export-import-corpuses/#exports","text":"OpenContracts support both exporting and importing corpuses. This functionality is disabled on the public demo as it can be bandwidth intensive. If you want to experiment with these features on your own, you'll see the export action when you right-click on a corpus: You can access your exports from the user dropdown menu in the top right corner of the screen. Once your export is complete, you should be able to download a zip containing all the documents, their PAWLs layers, and the corpus data you created - including all annotations.","title":"Exports"},{"location":"walkthrough/advanced/export-import-corpuses/#imports","text":"If you've enabled corpus imports (see the frontend env file for the boolean toggle to do this - it's REACT_APP_ALLOW_IMPORTS ), you'll see an import action when you click the action button on the corpus page.","title":"Imports"},{"location":"walkthrough/advanced/fork-a-corpus/","text":"To Fork or Not to Fork? \u00b6 One of the amazing things about Open Source collaboration is you can stand on the shoulder of giants - we can share techniques and data and collectively achieve what we could never do alone. OpenContracts is designed to make it super easy to share and re-use annotation data. In OpenContracts, we introduce the concept of \"forking\" a corpus - basically creating a copy of public or private corpus, complete with its documents and annotations, which you can edit and tweak as needed. This opens up some interesting possibilities. For example, you might have a base corpus with annotations common to many types of AI models or annotation projects which you can fork as needed and layer task or domain-specific annotations on top of. Fork a Corpus \u00b6 Forking a corpus is easy. Again, right-click on a corpus to bring up the context menu. You'll see an entry to \"Fork Corpus\": Click on it to start a fork. You should see a confirmation in the top right of the screen: Once the fork is complete, the next time you go to your Corpus page, you'll see a new Corpus with a Fork icon in the icon bar at the bottom. If you hover over it, you'll be able to see a summary of the corpus it was forked from. This is tracked in the database, so, long-term, we'd like to have corpus version control similar to how git works:","title":"Fork a Corpus"},{"location":"walkthrough/advanced/fork-a-corpus/#to-fork-or-not-to-fork","text":"One of the amazing things about Open Source collaboration is you can stand on the shoulder of giants - we can share techniques and data and collectively achieve what we could never do alone. OpenContracts is designed to make it super easy to share and re-use annotation data. In OpenContracts, we introduce the concept of \"forking\" a corpus - basically creating a copy of public or private corpus, complete with its documents and annotations, which you can edit and tweak as needed. This opens up some interesting possibilities. For example, you might have a base corpus with annotations common to many types of AI models or annotation projects which you can fork as needed and layer task or domain-specific annotations on top of.","title":"To Fork or Not to Fork?"},{"location":"walkthrough/advanced/fork-a-corpus/#fork-a-corpus","text":"Forking a corpus is easy. Again, right-click on a corpus to bring up the context menu. You'll see an entry to \"Fork Corpus\": Click on it to start a fork. You should see a confirmation in the top right of the screen: Once the fork is complete, the next time you go to your Corpus page, you'll see a new Corpus with a Fork icon in the icon bar at the bottom. If you hover over it, you'll be able to see a summary of the corpus it was forked from. This is tracked in the database, so, long-term, we'd like to have corpus version control similar to how git works:","title":"Fork a Corpus"},{"location":"walkthrough/advanced/generate-graphql-schema-files/","text":"Generating GraphQL Schema Files \u00b6 Open Contracts uses Graphene to provide a rich GraphQL endpoint, complete with the GraphiQL query application. For some applications, you may want to generate a GraphQL schema file in SDL or json. On example use case is if you're developing a frontend you want to connect to OpenContracts, and you'd like to autogenerate Typescript types from a GraphQL Schena. To generate a GraphQL schema file, run your choice of the following commands. For an SDL file: $ docker-compose -f local.yml run django python manage.py graphql_schema --schema config.graphql.schema.schema --out schema.graphql For a JSON file: $ docker-compose -f local.yml run django python manage.py graphql_schema --schema config.graphql.schema.schema --out schema.json You can convert these to TypeScript for use in a frontend (though you'll find this has already been done for the React- based OpenContracts frontend) using a tool like this .","title":"Generate GraphQL Schema Files"},{"location":"walkthrough/advanced/generate-graphql-schema-files/#generating-graphql-schema-files","text":"Open Contracts uses Graphene to provide a rich GraphQL endpoint, complete with the GraphiQL query application. For some applications, you may want to generate a GraphQL schema file in SDL or json. On example use case is if you're developing a frontend you want to connect to OpenContracts, and you'd like to autogenerate Typescript types from a GraphQL Schena. To generate a GraphQL schema file, run your choice of the following commands. For an SDL file: $ docker-compose -f local.yml run django python manage.py graphql_schema --schema config.graphql.schema.schema --out schema.graphql For a JSON file: $ docker-compose -f local.yml run django python manage.py graphql_schema --schema config.graphql.schema.schema --out schema.json You can convert these to TypeScript for use in a frontend (though you'll find this has already been done for the React- based OpenContracts frontend) using a tool like this .","title":"Generating GraphQL Schema Files"},{"location":"walkthrough/advanced/run-gremlin-analyzer/","text":"Introduction to Gremlin Integration \u00b6 OpenContracts integrates with a powerful NLP engine called Gremlin Engine (\"Gremlin\"). If you run a Gremlin analyzer on a Corpus, it will create annotations of its own that you can view and export (e.g. automatically applying document labels or labeling parties, dates, and places, etc.). It's meant to provide a consistent API to deliver and render NLP and machine learning capabilities to end-users. As discussed in the configuration section, you need to install Gremlin Analyzers through the admin dashboard. Once you've installed Gremlin Analyzers, however, it's easy to apply them. Using an Installed Gremlin Analyzer \u00b6 If analysis capabilities are enabled for instance , when you right-click on a Corpus, you'll see an option to \"Analyze Corpus\": Clicking on this item will bring up a dialog where you can browse available analyzers: Select one and hit \"Analyze\" to submit a corpus for processing. When you go to the Analysis tab of your Corpus now, you'll see the analysis. Most likely, if you just clicked there, it will say processing: When the Analysis is complete, you'll see a summary of the number of labels and annotations applied by the analyzer: Note on Processing Time \u00b6 Large Corpuses of hundreds of documents can take a long time to process (10 minutes or more). It's hard to predict processing time up front, because it's dependent on the number of total pages and the specific analysis being performed. At the moment, there is not a great mechanism in place to detect and handle failures in a Gremlin analyzer and reflect this in OpenContracts. It's on our roadmap to improve this integration. In the meantime, the example analyzers we've released with Gremlin should be very stable, so they should run predictably. Viewing the Outputs \u00b6 Once an Analysis completes, you'll be able to browse the annotations from the analysis in several ways. First, they'll be available in the \"Annotation\" tab, and you can easily filter to annotations from a specific analyzer. Second, when you load a Document, in the Annotator view, there's a small widget in the top of the annotator that has three downwards-facing arrows and says \"Human Annotation Mode\". Click on the arrows open a tray showing the analyses applied to this document. Click on an analysis to load the annotations and view them in the document. Note : You can delete an analysis, but you cannot edit it. The annotations are machine-created and cannot be edited by human users.","title":"Run a Gremlin Analyzer"},{"location":"walkthrough/advanced/run-gremlin-analyzer/#introduction-to-gremlin-integration","text":"OpenContracts integrates with a powerful NLP engine called Gremlin Engine (\"Gremlin\"). If you run a Gremlin analyzer on a Corpus, it will create annotations of its own that you can view and export (e.g. automatically applying document labels or labeling parties, dates, and places, etc.). It's meant to provide a consistent API to deliver and render NLP and machine learning capabilities to end-users. As discussed in the configuration section, you need to install Gremlin Analyzers through the admin dashboard. Once you've installed Gremlin Analyzers, however, it's easy to apply them.","title":"Introduction to Gremlin Integration"},{"location":"walkthrough/advanced/run-gremlin-analyzer/#using-an-installed-gremlin-analyzer","text":"If analysis capabilities are enabled for instance , when you right-click on a Corpus, you'll see an option to \"Analyze Corpus\": Clicking on this item will bring up a dialog where you can browse available analyzers: Select one and hit \"Analyze\" to submit a corpus for processing. When you go to the Analysis tab of your Corpus now, you'll see the analysis. Most likely, if you just clicked there, it will say processing: When the Analysis is complete, you'll see a summary of the number of labels and annotations applied by the analyzer:","title":"Using an Installed Gremlin Analyzer"},{"location":"walkthrough/advanced/run-gremlin-analyzer/#note-on-processing-time","text":"Large Corpuses of hundreds of documents can take a long time to process (10 minutes or more). It's hard to predict processing time up front, because it's dependent on the number of total pages and the specific analysis being performed. At the moment, there is not a great mechanism in place to detect and handle failures in a Gremlin analyzer and reflect this in OpenContracts. It's on our roadmap to improve this integration. In the meantime, the example analyzers we've released with Gremlin should be very stable, so they should run predictably.","title":"Note on Processing Time"},{"location":"walkthrough/advanced/run-gremlin-analyzer/#viewing-the-outputs","text":"Once an Analysis completes, you'll be able to browse the annotations from the analysis in several ways. First, they'll be available in the \"Annotation\" tab, and you can easily filter to annotations from a specific analyzer. Second, when you load a Document, in the Annotator view, there's a small widget in the top of the annotator that has three downwards-facing arrows and says \"Human Annotation Mode\". Click on the arrows open a tray showing the analyses applied to this document. Click on an analysis to load the annotations and view them in the document. Note : You can delete an analysis, but you cannot edit it. The annotations are machine-created and cannot be edited by human users.","title":"Viewing the Outputs"}]}