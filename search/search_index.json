{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenContracts","text":""},{"location":"#the-free-and-open-source-document-analysis-platform","title":"The Free and Open Source Document Analysis Platform","text":"CI/CD Meta"},{"location":"#what-does-it-do","title":"What Does it Do?","text":"<p>OpenContracts is an Apache-2 Licensed software application to label, share and search annotate documents. It's  designed specifically to label documents with complex layouts such as contracts, scientific papers, newspapers, etc.</p> <p></p> <p>When combine with a NLP processing engine like Gremlin Engine (another of our open source projects), OpenContracts not only lets humans collaborate on and share document annotations, it also can analyze and export data from contracts using state-of-the-art NLP technology.</p>"},{"location":"#why-does-it-exist","title":"Why Does it Exist?","text":"<p>The OpenContracts stack is designed to provide a cutting edge frontend experience while providing access to the incredible machine learning and natural language processing capabilities of Python. For this reason, our frontend is based on React. We use a GraphQL API to connect it to a django-based backend. Django is a incredibly mature, battle-tested framework that is written in Python, so integrating all the amazing Python-based AI and NLP libraries out there is super easy.</p> <p>We'd like to give credit to AllenAI's PAWLs project for our document annotating component. We rewrote most of the code base and replaced their backend entirely, so it was hard to keep , but we believe in giving credit where it's due! We are relying on their document parser, however, as it produces a really excellent text and x-y coordinate layer that we'd encourage others to use as well in similar applications that require you to interact with complex text layouts.</p>"},{"location":"#limitations","title":"Limitations","text":"<p>At the moment, it only works with PDFs. In the future, it will be able to convert other document types to PDF for storage and labeling. PDF is an excellent format for this as it introduces a consistent, repeatable format which we can use to generate a text and x-y coordinate layer from scratch. Formats like .docx and .html are too complex and varied to provide an easy, consistent format. Likewise, the output quality of many converters and tools is sub-par and these tools can produce very different document structures for the same inputs.</p>"},{"location":"#about-opensourcelegal","title":"About OpenSource.Legal","text":"<p>OpenSource.Legal believes that the effective, digital transformation of the legal services industry and the execution of \"the law\", broadly speaking, requires shared solutions and tools to solve some of the problems that are common to almost every legal workflow. The current splintering of service delivery into dozens of incompatible platforms with limited configurations threatens to put software developers and software vendors in the driver seat of the industry. We firmly believe that lawyers and legal engineers, armed with easily configurable and extensible tools can much more effectively design the workflows and user experiences that they need to deliver and scale their expertise.</p> <p>Visit us at https://opensource.legal for a directory of open source legal projects and an overview of our projects.</p>"},{"location":"acknowledgements/","title":"Acknowledgements","text":"<p>OpenContracts is built in part on top of the PAWLs project frontend. We have made extensive changes, however, and plan to remove even more of the original PAWLs codebase, particularly their state management, as it's currently duplucitive of the Apollo state store we use throughout the application. That said, PAWLs was the inspiration for how we handle text extraction, and we're planning to continue using their PDF rendering code. We are also using PAWLs' pre-processing script, which is based on Grobid.</p> <p>We should also thank the Grobid project, which was clearly a source of inspiration for PAWLs and an extremely impressive tool. Grobid is designed more for medical and scientific papers, but, nevertheless, offers a tremendous amount of inspiration and examples for the legal world to borrow. Perhaps there is an opportunity to have a unified tool in that respect.</p> <p>Finally, let's not forget Tesseract, the OCR engine that started its life as an HP research project in the 1980s before being taken over by Google in the early aughts and finally becoming an independent project in 2018. Were it not for the excellent, free OCR provided by Tesseract, we'd have to rely on commercial OCR tech, which would make this kind of opensource, free project prohibitively expensive. Thanks to the many, many people who've made free OCR possible over the nearly 40 years Tesseract has been under development.</p>"},{"location":"philosophy/","title":"Philosophy","text":""},{"location":"philosophy/#dont-repeat-yourself","title":"Don't Repeat Yourself","text":"<p>OpenContracts is designed not only be a powerful document analysis and annotation platform, it's also envisioned as a way to embrace the DRY (Don't Repeat Yourself) principle for legal and legal engineering. You can make a corpus, along with all of its labels, documents and annotations \"public\" (currently, you must do this via a GraphQL mutation).</p> <p>Once something is public, it's read-only for everyone other than its original creator. People with read-only access can \"clone\" the corpus to create a private copy of the corpus, its documents and its annotations. They can then edit the annotations, add to them, export them, etc. This lets us work from previous document annotations and re-use labels and training data.</p>"},{"location":"quick-start/","title":"Quick Start (For use on your local machine)","text":"<p>This guide is for people who want to quickly get started using the application and aren't interested in hosting it online for others to use. You'll get a default, local user with admin access. We recommend you change the user password after completing this tutorial. We assume you're using Linux or Max OS, but you could do this on Windows too, assuming you have docker compose and docker installed. The commands to create directories will be different on Windows, but the git, docker and docker-compose commands should all be the same.</p>"},{"location":"quick-start/#step-1-clone-this-repo","title":"Step 1: Clone this Repo","text":"<p>Clone the repository into a local directory of your choice. Here, we assume you are using a folder called source in your user's home directory:</p> <pre><code>    $ cd ~\n    $ mkdir source\n    $ cd source\n    $ git clone https://github.com/JSv4/OpenContracts.git\n</code></pre>"},{"location":"quick-start/#step-2-build-the-stack","title":"Step 2: Build the Stack","text":"<p>Change into the directory of the repository you just cloned, e.g.:</p> <pre><code>    cd OpenContracts\n</code></pre> <p>Now, you need to build the docker compose stack. IF you are okay with the default username and password, and, most importantly, you are NOT PLANNING TO HOST THE APPLICATION online, the default, local settings are sufficient and no configuration is required. If you want to change the</p> <pre><code>    $ docker-compose -f local.yml build\n</code></pre>"},{"location":"quick-start/#step-3-choose-frontend-deployment-method","title":"Step 3 Choose Frontend Deployment Method","text":"<p>Option 1 Use \"Fullstack\" Profile in Docker Compose</p> <p>If you're not planning to do any frontend development, the easiest way to get started with OpenContracts is to  just type:</p> <pre><code>    docker-compose -f local.yml --profile fullstack up\n</code></pre> <p>This will start docker compose and add a container for the frontend to the stack. </p> <p>Option 2 Use Node to Deploy Frontend</p> <p>If you plan to actively develop the frontend in the  /frontend folder, you can just point your favorite  typescript ID to that directory and then run:</p> <pre><code>yarn install\n</code></pre> <p>and </p> <pre><code>yarn start\n</code></pre> <p>to bring up the frontend. Then you can edit the frontend code as desired and have it hot reload as you'd expect for a  React app.</p> <p>Congrats! You have OpenContracts running. </p>"},{"location":"quick-start/#step-4-login-and-start-annotating","title":"Step 4: Login and Start Annotating","text":"<p>If you go to <code>http://localhost:3000</code> in your browser, you'll see the login page. You can login with the default username and password. These are set in the environment variable file you can find in the <code>./.envs/.local/' directory. In that  directory, you'll see a file called</code>.django`. Backend specific configuration variables go in there. </p> <p>NOTE: The frontend is at port 3000, not 8000, so don't forget to use http://localhost:3000 for frontend access. We  have an open issue to add a redirect from the backend root page - http://localhost:8000/ - to http://localhost:3000.</p> <p>Caveats</p> <p>The quick start local config is designed for use on a local machine, not for access over the Internet or a network. It uses the local disk for storage (not AWS), and Django's built-i</p>"},{"location":"requirements/","title":"System Requirements","text":""},{"location":"requirements/#system-requirements","title":"System Requirements","text":"<p>You will need Docker and Docker Compose installed to run Open Contracts. We've developed and run the application a Linux x86_64 environment. We haven't tested on Windows, and it's known that celery is not supported on Windows. For this reason, we do not recommend deployment on Windows. If you must run on a Windows machine, consider using a virtual machine or using the Windows Linux Subsystem.</p> <p>If you need help setting up Docker, we recommend Digital Ocean's setup guide. Likewise, if you need assistance setting up Docker Compose, Digital Ocean's guide is excellent.</p>"},{"location":"architecture/asynchronous-processing/","title":"Asynchronous processing","text":""},{"location":"architecture/asynchronous-processing/#asynchronous-tasks","title":"Asynchronous Tasks","text":"<p>OpenContracts makes extensive use of celery, a powerful, mature python framework for distributed and asynchronous processing. Out-of-the-box, dedicated celeryworkers are configured in the docker compose stack to handle computationally-intensive and long-running tasks like parsing documents, applying annotations to pdfs, creating exports, importing exports, and more.</p>"},{"location":"architecture/asynchronous-processing/#what-if-my-celery-queue-gets-clogged","title":"What if my celery queue gets clogged?","text":"<p>We are always working to make OpenContracts more fault-tolerant and stable. That said, due to the nature of the types of documents we're working with - pdfs - there is tremendous variation in what the parsers have to parse. Some documents are extremely long - thousands of pages or more - whereas other documents may have poor formatting, no text layers, etc.. In most cases, OpenContracts should be able to process the pdfs and make them compatible with our annotation tools. Sometimes, however, either due to unexpected issues or unexpected volume of documents, you may want to purge the queue of tasks to be processed by your celery workers. To do this, type:</p> <pre><code>sudo docker-compose -f local.yml run django celery -A config.celery_app purge\n</code></pre> <p>Be aware that this can cause some undesired effects for your users. For example, everytime a new document is uploaded, a Django signal kicks off the pdf preprocessor to produce the PAWLs token layer that is later annotated. If these tasks are in-queue and the queue is purged, you'll have documents that are not annotatable as they'll lack the PAWLS token layers. In such cases, we recommend you delete and re-upload the documents. There are ways to manually reprocess the pdfs, but we don't have a user-friendly way to do this yet.</p>"},{"location":"architecture/under-the-hood/","title":"Under the hood","text":""},{"location":"architecture/under-the-hood/#data-layers","title":"Data Layers","text":"<p>OpenContracts builds on the work that AllenAI did with PAWLs to create a consistent shared source of truth for data labeling and NLP algorithms, regardless of whether they are layout-aware, like LayoutLM or not, like BERT, Spacy or LexNLP. One of the challenges with natural language documents, particularly contracts is there are so many ways to structure any given file (e.g. .docx or .pdf) to represent exactly the same text. Even an identical document with identical formatting in a format like .pdf can have a significantly different file structure depending on what software was used to create it, the user's choices, and the software's own choices in deciding how to structure its output.</p> <p>PAWLs and OpenContracts attempt to solve this by sending every document through a processing pipeline that provides a uniform and consistent way of extracting and structuring text and layout information. Using the parsing engine of Grobid and the open source OCR engine Tesseract, every single document is re-OCRed (to produce a consistent output for the same inputs) and then the \"tokens\" (text surrounded on all sides by whitespace - typically a word) in the OCRed document are stored as JSONs with their page and positional information. In OpenContracts, we refer to this JSON layer that combines text and positional data as the \"PAWLs\" layer. We use the PAWLs layer to build the full text extract from the document as well and store this as the \"text layer\".</p> <p>Thus, in OpenContracts, every document has three files associated with it - the original pdf, a json file (the \"PAWLs layer\"), and a text file (the \"text layer\"). Because the text layer is built from the PAWLs layer, we can easily translate back and forth from text to positional information - e.g. given the start and end of a span of text the text layer, we can accurately say which PAWLs tokens the span includes, and, based on that, the x,y position of the span in the document.</p> <p>This lets us take the outputs of many NLP libraries which typically produce only start and stop ranges and layer them perfectly on top of the original pdf. With the PAWLs tokens as the source of truth, we can seamlessly transition from text only to layout-aware text.</p>"},{"location":"architecture/under-the-hood/#limitations","title":"Limitations","text":"<p>OCR is not perfect. By only accepting pdf inputs and OCRing every document, we do ignore any text embedded in the pdf. To the extent that text was exported accurately from whatever tool was used to write the document, this introduces some potential loss of fidelity - e.g. if you've ever seen an OCR engine mistake an 'O' or a 0 or 'I' for a '1' or something like that. Typically, however, the instance of such errors is fairly small, and it's a price we have to pay for the power of being able to effortlessly layer NLP outputs that have no layout awareness on top of complex, visual layouts.</p>"},{"location":"configuration/add-users/","title":"Add Users","text":""},{"location":"configuration/add-users/#adding-more-users","title":"Adding More Users","text":"<p>You can use the same User admin page described above to create new users. Alternatively, go back to the main admin page <code>http://localhost:8000/admin</code> and, under the User section, click the \"+Add\" button:</p> <p></p> <p>Then, follow the on-screen instructions:</p> <p></p> <p>When you're done, the username and password you provided can be used to login.</p> <p>OpenContracts is currently not built to allow users to self-register unless you use the Auth0 authentication. When managing users yourself, you'll need to add, remove and modify users via the admin panels.</p>"},{"location":"configuration/choose-an-authentication-backend/","title":"Configure Authentication Backend","text":""},{"location":"configuration/choose-an-authentication-backend/#select-authentication-system-via-env-variables","title":"Select Authentication System via Env Variables","text":"<p>For authentication and authorization, you have two choices. 1. You can configure an Auth0 account and use Auth0 to authenticate users, in which case anyone    who is permitted to authenticate via your auth0 setup can login and automatically get an account, 2. or, you can require a username and password for each user and our OpenContracts backend can provide user    authentication and authorization. Using the latter option, there is no currently-supported sign-up method, you'll    need to use the admin dashboard (See \"Adding Users\" section).</p>"},{"location":"configuration/choose-an-authentication-backend/#auth0-auth-setup","title":"Auth0 Auth Setup","text":"<p>You need to configure three, separate applications on Auth0's platform:</p> <ol> <li>Configure the SPA as an application. You'll need the App Client ID.</li> <li>Configure the API. You'll need API Audience.</li> <li>Configure a M2M application to access the Auth0 Management API. This is used to fetch user details.    You'll need the API_ID for the M2M application and the Client Secret for the M2M app.</li> </ol> <p>You'll also need your Auth0 tenant ID (assuming it's the same for all three applications, though you could, in theory, host them in different tenants).  These directions are not comprehensive, so, if you're not familiar with Auth0, we recommend you disable Auth0 for the time being and use username and password.</p> <p>To enable and configure Auth0 Authentication, you'll need to set the following env variables in your .env file (the .django file in <code>.envs/.production</code> or <code>.envs/.local</code>, depending on your target environment). Our sample .envs only show these fields in the .production sample, but you could use them in the .local env file too:</p> <ol> <li><code>USE_AUTH0</code> - set to <code>true</code> to enable Auth0</li> <li><code>AUTH0_CLIENT_ID</code> - should be the client ID configured on Auth0</li> <li><code>AUTH0_API_AUDIENCE</code> - Configured API audience</li> <li><code>AUTH0_DOMAIN</code> - domain of your configured Auth0 application</li> <li><code>AUTH0_M2M_MANAGEMENT_API_SECRET</code> - secret for the auth0 Machine to Machine (M2M) API</li> <li><code>AUTH0_M2M_MANAGEMENT_API_ID</code> - ID for Auth0 Machine to Machine (M2M) API</li> <li><code>AUTH0_M2M_MANAGEMENT_GRANT_TYPE</code> - set to <code>client_credentials</code></li> </ol>"},{"location":"configuration/choose-an-authentication-backend/#detailed-explanation-of-auth0-implementation","title":"Detailed Explanation of Auth0 Implementation","text":"<p>To get Auth0 to work nicely with Graphene, we modified the graphql_jwt backend to support syncing  remote user metadata with a local user similar to the default, django <code>RemoteUserMiddleware</code>.  We're keeping the graphql_jwt graphene middleware in its entirety as it fetches the  token and then passes it along to django authentication *backend. That django backend  is what we're modifying to decode the jwt token against Auth0 settings and then check to see if local user exists, and, if not, create it.</p> <p>Here's the order of operations in the original Graphene backend provided by graphql_jwt:</p> <ol> <li>Backend's <code>authenticate</code> method is called from the graphene middleware via django (from django.contrib.auth    import authenticate)</li> <li>token is retrieved via .utils get_credentials</li> <li>if token is not None, get_user_by_token in shortcuts module is called<ol> <li>\"Payload\" is retrieved via utils.get_payload</li> <li>User is requested via utils.get_user_by_payload</li> <li>username is retrieved from payload via <code>auth0_settings.JWT_PAYLOAD_GET_USERNAME_HANDLER</code></li> <li>user object is retrieved via <code>auth0_settings.JWT_GET_USER_BY_NATURAL_KEY_HANDLER</code></li> </ol> </li> </ol> <p>We modified a couple things:</p> <ol> <li>The decode method called in 3(a) needs to be modified to decode with Auth0 secrets and settings.</li> <li>get_user_by_payload needs to be modified in several ways:<ol> <li>user object must use <code>RemoteUserMiddleware</code> logic and, if everything from auth0 decodes properly,       check to see if user with e-mail exists and, if not, create it. Upon completion of this,       try to sync user data with auth0. 2) return created or retrieved user object as original method did</li> </ol> </li> </ol>"},{"location":"configuration/choose-an-authentication-backend/#django-based-authentication-setup","title":"Django-Based Authentication Setup","text":"<p>The only thing you need to do for this is toggle the two auth0-related environment variables: 1. For the backend environment, set <code>USE_AUTH0=False</code> in your environment (either via an environment variable file or    directly in your environment via the console). 2. For the frontend environment, set <code>REACT_APP_USE_AUTH0=false</code> in your environment (either via an environment variable file or    directly in your environment via the console).</p> <p>Note</p> <p>As noted elsewhere, users cannot sign up on their own. You need to log into the admin dashboard - e.g. <code>http://localhost:8000/admin</code> - and add users manually.</p>"},{"location":"configuration/choose-and-configure-docker-stack/","title":"Choose and Configure Docker Compose Stack","text":""},{"location":"configuration/choose-and-configure-docker-stack/#deployment-options","title":"Deployment Options","text":"<p>OpenContracts is designed to be deployed using docker-compose. You can run it locally or in a production environment. Follow the instructions below for a local environment if you just want to test it or you want to use it for yourself and don't intend to make the application available to other users via the Internet.</p>"},{"location":"configuration/choose-and-configure-docker-stack/#local-deployment","title":"Local Deployment","text":""},{"location":"configuration/choose-and-configure-docker-stack/#quick-start-with-default-settings","title":"Quick Start with Default Settings","text":"<p>A \"local\" deployment is deployed on your personal computer and is not meant to be accessed over the Internet. If you don't need to configure anything, just follow the quick start guide above to get up and running with a local deployment without needing any further configuration.</p>"},{"location":"configuration/choose-and-configure-docker-stack/#setup-env-files","title":"Setup .env Files","text":""},{"location":"configuration/choose-and-configure-docker-stack/#backend","title":"Backend","text":"<p>After cloning this repo to a machine of your choice, create a folder for your environment files in the repo root. You'll need <code>./.envs/.local/.django</code> and <code>./.envs/.local/.postgres</code> Use the samples in <code>./documentation/sample_env_files/local</code> as guidance. NOTE, you'll need to replace the placeholder passwords and users where noted, but, otherwise, minimal config should be required.</p>"},{"location":"configuration/choose-and-configure-docker-stack/#frontend","title":"Frontend","text":"<p>In the <code>./frontend</code> folder, you also need to create a single .env file which holds your configurations for your login method as well as certain feature switches (e.g. turn off imports). We've included a sample using auth0 and another sample using django's auth backend. Local vs production deployments are essentially the same, but the root url of the backend will change from localhost to whereever you're hosting the application in production.</p>"},{"location":"configuration/choose-and-configure-docker-stack/#build-the-stack","title":"Build the Stack","text":"<p>Once your .env files are setup, build the stack using docker-compose:</p> <p><code>$ docker-compose -f local.yml build</code></p> <p>Then, run migrations (to setup the database):</p> <p><code>$ docker-compose -f local.yml run django python manage.py migrate</code></p> <p>Then, create a superuser account that can log in to the admin dashboard (in a local deployment this is available at <code>http://localhost:8000/admin</code>) by typing this command and following the prompts:</p> <pre><code>$ docker-compose -f local.yml run django python manage.py createsuperuser\n</code></pre> <p>Finally, bring up the stack:</p> <pre><code>$ docker-compose -f local.yml up\n</code></pre> <p>You should now be able to access the OpenContracts frontend by visiting <code>http://localhost:3000</code>.</p>"},{"location":"configuration/choose-and-configure-docker-stack/#production-environment","title":"Production Environment","text":"<p>The production environment is designed to be public-facing and exposed to the Internet, so there are quite a number more configurations required than a local deployment, particularly if you use an AWS S3 storage backend or the Auth0 authentication system.</p> <p>After cloning this repo to a machine of your choice, configure the production .env files as described above.</p> <p>You'll also need to configure your website url. This needs to be done in a few places.</p> <p>First, in <code>opencontractserver/contrib/migrations</code>, you'll fine a file called <code>0003_set_site_domain_and_name.py</code>. BEFORE  running any of your migrations, you should modify the <code>domain</code> and <code>name</code> defaults you'll fine in <code>update_site_forward</code>:</p> <pre><code>def update_site_forward(apps, schema_editor):\n \"\"\"Set site domain and name.\"\"\" Site = apps.get_model(\"sites\", \"Site\") Site.objects.update_or_create( id=settings.SITE_ID, defaults={ \"domain\": \"opencontracts.opensource.legal\", \"name\": \"OpenContractServer\", }, )\n</code></pre> <p>and <code>update_site_backward</code>:</p> <pre><code>def update_site_backward(apps, schema_editor):\n \"\"\"Revert site domain and name to default.\"\"\" Site = apps.get_model(\"sites\", \"Site\") Site.objects.update_or_create( id=settings.SITE_ID, defaults={\"domain\": \"example.com\", \"name\": \"example.com\"} )\n</code></pre> <p>Finally, don't forget to configure Treafik, the router in the docker-compose stack that exposes different containers to end-users depending on the route (url) received you need to update the Treafik file here.</p> <p>If you're using Auth0, see the Auth0 configuration section.</p> <p>If you're using AWS S3 for file storage, see the AWS configuration section. NOTE, the underlying django library that provides cloud storage, django-storages, can also work with other cloud providers such as Azure and GCP. See the django storages library docs for more info.</p> <pre><code>$ docker-compose -f production.yml build\n</code></pre> <p>Then, run migrations (to setup the database):</p> <pre><code>$ docker-compose -f production.yml run django python manage.py migrate`\n</code></pre> <p>Then, create a superuser account that can log in to the admin dashboard (in a production deployment this is available at the url set in your env file as the <code>DJANGO_ADMIN_URL</code>) by typing this command and following the prompts:</p> <pre><code>$ docker-compose -f production.yml run django python manage.py createsuperuser\n</code></pre> <p>Finally, bring up the stack:</p> <pre><code>$ docker-compose -f production.yml up\n</code></pre> <p>You should now be able to access the OpenContracts frontend by visiting <code>http://localhost:3000</code>.</p>"},{"location":"configuration/choose-and-configure-docker-stack/#env-file-configurations","title":"ENV File Configurations","text":"<p>OpenContracts is configured via .env files. For a local deployment, these should go in <code>.envs/.local</code>. For production, use <code>.envs/.production</code>. Sample .envs for each deployment environment are provided in <code>documentation/sample_env_files</code>.</p> <p>The local configuration should let you deploy the application on your PC without requiring any specific configuration. The production configuration is meant to provide a web application and requires quite a bit more configuration and knowledge of web apps.</p>"},{"location":"configuration/choose-and-configure-docker-stack/#include-gremlin","title":"Include Gremlin","text":"<p>If you want to include a Gremlin analyzer, use <code>local_deploy_with_gremlin.yml</code> or <code>production_deploy_with_gremlin.yml</code> instead of <code>local.yml</code> or <code>production.yml</code>, respectively. All other parts of the tutorial are the same.</p>"},{"location":"configuration/choose-storage-backend/","title":"Configure Storage Backend","text":""},{"location":"configuration/choose-storage-backend/#select-and-setup-storage-backend","title":"Select and Setup Storage Backend","text":"<p>You can use Amazon S3 as a file storage backend (if you set the env flag <code>USE_AWS=True</code>, more on that below), or you can use the local storage of the host machine via a Docker volume.</p>"},{"location":"configuration/choose-storage-backend/#aws-storage-backend","title":"AWS Storage Backend","text":"<p>If you want to use AWS S3 to store files (primarily pdfs, but also exports, tokens and txt files), you will need an Amazon AWS account to setup S3. This README does not cover the AWS side of configuration, but there  are a number of tutorials and guides to getting AWS configured to be used with a django project.</p> <p>Once you have an S3 bucket configured, you'll need to set the following env variables in your .env file (the <code>.django</code> file in <code>.envs/.production</code> or <code>.envs/.local</code>, depending on your target environment). Our sample .envs only show these fields in the .production samples, but you could use them in the .local env file too.</p> <p>Here the variables you need to set to enable AWS S3 storage:</p> <ol> <li><code>USE_AWS</code> - set to <code>true</code> since you're using AWS, otherwise the backend will use a docker volume for storage.</li> <li><code>DJANGO_AWS_ACCESS_KEY_ID</code> - the access key ID created by AWS when you set up your IAM user (see tutorials above).</li> <li><code>DJANGO_AWS_SECRET_ACCESS_KEY</code> - the secret access key created by AWS when you set up your IAM user    (see tutorials above)</li> <li><code>DJANGO_AWS_STORAGE_BUCKET_NAME</code> - the name of the AWS bucket you created to hold the files.</li> <li><code>DJANGO_AWS_S3_REGION_NAME</code> - the region of the AWS bucket you configured.</li> </ol>"},{"location":"configuration/choose-storage-backend/#django-storage-backend","title":"Django Storage Backend","text":"<p>Setting <code>USE_AWS=false</code> will use the disk space in the django container. When using the local docker compose stack, the celery workers and django containers share the same disk, so this works fine. Our production configuration would not work properly with <code>USE_AWS=false</code>, however, as each container has its own disk.</p>"},{"location":"configuration/configure-admin-users/","title":"Configure Admin Users","text":""},{"location":"configuration/configure-admin-users/#gremlin-admin-dashboard","title":"Gremlin Admin Dashboard","text":"<p>Gremlin's backend is built on Django, which has its own powerful admin dashboard. This dashboard is not meant for end-users and should only be used by admins. You can access the admin dashboard by going to the <code>/admin</code> page - e,g, <code>opencontracts.opensource.legal/admin</code> or <code>http://localhost:8000/admin</code>. For the most part, you shouldn't need to use the admin dashboard and should only go in here if you're experience errors or unexpected behavior and want to look at the detailed contents of the database to see if it sheds any light on what's happening with a give corpus, document, etc.</p> <p>By default, Gremlin creates an admin user for you. If you don't specify the username and password in your environment on first boot, it'll use system defaults. You can customize the default username and password via environment variables or after the system boots using the admin dash.</p>"},{"location":"configuration/configure-admin-users/#configure-username-and-password-prior-to-first-deployment","title":"Configure Username and Password Prior to First Deployment","text":"<p>If the variable <code>DJANGO_SUPERUSER_USERNAME</code> is set, that will be the default admin user created on startup (the first time your run <code>docker-compose -f local.yml up</code>). The repo ships with a default superuser username of <code>admin</code>. The default password is set using the <code>DJANGO_SUPERUSER_PASSWORD</code> variable. The environment files for local deployments (but not production) include a default password of <code>Openc0ntracts_def@ult</code>. You should change this in the environment file before the first start OR, follow the instructions below to change it after the first start.</p> <p>If you modify these environment variables in the environment file BEFORE running the docker-compose <code>up</code> command for the first time, your initial superuser will have the username, email and/or password you specify. If you don't modify the defaults, you can change them after you have created them via the admin dashboard (see below).</p>"},{"location":"configuration/configure-admin-users/#after-first-deployment-via-admin-dashboard","title":"After First Deployment via Admin Dashboard","text":"<p>Once the default superuser has been created, you'll need to use the admin dashboard to modify it.</p> <p>To manage users, including changing the password, you'll need to access the backend admin dashboard. OpenContracts is built on Django, which ships with Django Admin, a tool to manage low-level object data and users. It doesn't provide the rich, document focused UI/UX our frontend does, but it does let you edit and delete objects created on the frontend if, for any reason, you are unable to fix something done by a frontend user (e.g. a corrupt file is uploaded and cannot be parsed or rendered properly on the frontend).</p> <p>To update your users, first login to the admin panel:</p> <p></p> <p>Then, in the lefthand navbar, find the entry for \"Users\" and click on it</p> <p></p> <p>Then, you'll see a list of all users for this instance. You should see your admin user and an \"Anonymous\" user. The Anonymous user is required for public browsing of objcets with their <code>is_public</code> field set to True. The Anonymous user cannot see other objects.</p> <p></p> <p>Click on the admin user to bring up the detailed user view:</p> <p></p> <p>Now you can click the \"WHAT AM I CALLED\" button to bring up a dialog to change the user password.</p>"},{"location":"configuration/configure-gremlin/","title":"Configure Gremlin Analyzer","text":"<p>Gremlin is a separate project by OpenSource Legal to provide a standard API to access NLP capabilities. This lets us wrap multiple NLP engines / techniques in the same API which lets us build tools that can readily consume the outputs of very different NLP libraries (etc. a Transformers-based model like BERT, and tools like SPACY and LexNLP can be deployed on Gremlin and the outputs from all three can readily be rendered in OpenContracts).</p> <p>OpenContracts is designed to work with Gremlin out-of-the-box. We have a sample compose yaml file showing how to do this on a local machine <code>local_deploy_with_gremlin.yaml</code> and as a web-facing application <code>production_deploy_with_gremlin.yaml</code>.</p> <p>When you add a new Gremlin Engine to the database, OpenContracs will automatically query it for its installed analyzers and labels. These will then be available within OpenContracts, and you can use an analyzer to analyze any OpenContracts corpus.</p> <p>While we have plans to automatically \"install\" the default Gremlin on first boot, currently you must manually go into the OpenContracts admin dash and add the Gremlin. Thankfully, this is an easy process:</p> <ol> <li>In your environment file, make sure you set <code>CALLBACK_ROOT_URL_FOR_ANALYZER</code><ol> <li>For local deploy, use <code>CALLBACK_ROOT_URL_FOR_ANALYZER=http://localhost:8000</code></li> <li>For production deploy, use <code>http://django:5000</code>. Why the change? Well, in our local       docker compose stack, the host the localhost and the django development server runs on port 8000. In       production, we want Gremlin to communicate with the OpenContracts container (\"django\") via its       hostname on the docker compose stack's network. The production OpenContracts container also uses       gunicorn on port 5000 instead of the development server on port 8000, so the port changes too.</li> </ol> </li> <li>Go to the admin page:    </li> <li>Click \"Add+\" in the Gremlin row to bring up the Add Gremlin Engine form. You just need to set the creator    Url fields (the url for our default config is <code>http://gremlinengine:5000</code>). If, for some reason, you don't want    the analyzer to be visible to any unauthenticated user, unselect the <code>is_public</code> box :    </li> <li>This will automatically kick off an install process that runs in the background. When it's complete, you'll see the    \"Install Completed\" Field change. It should take a second or two. At the moment, we don't handle errors in this    process, so, if it doesn't complete successfully in 30 seconds, there is probably a misconfiguration somewhere. We    plan to improve our error handling for these backend installation processes.</li> </ol> <p>Note, in our example implementations, Gremlin is NOT encrypted or API Key secured to outside traffic. It's not exposed to outside traffic either per our docker compose config, so this shouldn't be a major concern. If you do expose the container to the host via your Docker Compose file, you should ensure you run the traffic through Treafik and setup API Key authentication.</p>"},{"location":"development/documentation/","title":"Documentation","text":""},{"location":"development/documentation/#documentation-stack","title":"Documentation Stack","text":"<p>We're using mkdocs to render our markdown into pretty, bite-sized pieces. The markdown lives in <code>/docs</code> in our repo. If you want to work on the docs you'll need to install the requirements in <code>/requirements/docs.txt</code>.</p> <p>To have a live server while working on them, type:</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"development/documentation/#building-docs","title":"Building Docs","text":"<p>To build a html website from your markdown that can be uploaded to a webhost (or a GitHub Page), just type:</p> <pre><code>mkdocs build\n</code></pre>"},{"location":"development/documentation/#deploying-to-gh-page","title":"Deploying to GH Page","text":"<p>mkdocs makes it super easy to deploy your docs to a GitHub page.</p> <p>Just run:</p> <pre><code>mkdocs gh-deploy\n</code></pre>"},{"location":"development/environment/","title":"Dev Environment","text":"<p>We use Black and Flake8 for Python Code Styling. These are run via pre-commit before all commits. If you want to develop extensions or code based on OpenContracts, you'll need to setup pre-commit. First, make sure the requirements in <code>./requirements/local.txt</code> are installed in your local environment.</p> <p>Then, install pre-commit into your local git repo. From the root of the repo, run:</p> <p><pre><code> $ pre-commit install\n</code></pre> If you want to run pre-commit manually on all the code in the repo, use this command:</p> <pre><code> $ pre-commit run --all-files\n</code></pre> <p>When you commit changes to your repo or our repo as a PR, pre-commit will run and ensure your code follows our style guide and passes linting.</p>"},{"location":"development/frontend-notes/","title":"Frontend Notes","text":""},{"location":"development/frontend-notes/#responsive-layout","title":"Responsive Layout","text":"<p>The application was primarily designed to be viewed around 1080p. We've built in some quick and dirty (honestly, hacks) to display a usable layout at other resolutions. A more thorough redesign / refactor is in order, again if there's sufficient interest. What's available now should handle a lot of situations ok. If you find performance / layout is not looking great at your given resolution, try to use a desktop browser at a 1080p resolution.</p>"},{"location":"development/frontend-notes/#no-test-suite","title":"No Test Suite","text":"<p>As of our initial release, the test suite only tests the backend (and coverage is admittedly not as robust as we'd like). We'd like to add tests for the frontend, though this is a fairly large undertaking. We welcome any contributions on this front!</p>"},{"location":"development/test-suite/","title":"Test Suite","text":"<p>Our test suite is a bit sparse, but we're working to improve coverage on the backend. Frontend tests will likely take longer to implement. Our existing tests do test imports and a number of the utility functions for manipulating annotations. These tests are integrated in our GitHub actions.</p> <p>NOTE, use Python 3.10 or above as pydantic and certain pre-3.10 type annotations do not play well. using <code>from __future__ import annotations</code> doesn't always solve the problem, and upgrading to Python 3.10 was a lot easier than trying to figure out why the <code>from __future__</code> didn't behave as expected</p> <p>To run the tests, check your test coverage, and generate an HTML coverage report:</p> <pre><code> $ docker-compose -f local.yml run django coverage run -m pytest\n $ docker-compose -f local.yml run django coverage html\n $ open htmlcov/index.html\n</code></pre> <p>To run a specific test (e.g. test_analyzers):</p> <pre><code> $ sudo docker-compose -f local.yml run django python manage.py test opencontractserver.tests.test_analyzers --noinput\n</code></pre>"},{"location":"walkthrough/key-concepts/","title":"Key-Concepts","text":""},{"location":"walkthrough/key-concepts/#data-types","title":"Data Types","text":"<p>Text annotation data is divided into several concepts:</p> <ol> <li>Corpuses (or collections of documents). One document can be in multiple corpuses.</li> <li>Documents. Currently, these are PDFs ONLY.</li> <li>Annotations. These are either document-level annotations (the document type), text-level annotations (highlighted    text), or relationships (which apply a label between two annotations). Relationships are currently not    well-supported and may be buggy.</li> <li>Analyses. These groups of read-only annotations added by a Gremlin analyzer (see more on that below).</li> </ol>"},{"location":"walkthrough/key-concepts/#permissioning","title":"Permissioning","text":"<p>OpenContracts is built on top of the powerful permissioning framework for Django called <code>django-guardian</code>. Each GraphQL request can add a field to annotate the object-level permissions the current user has for a given object, and the frontend relies on this to determine whether to make some objects and pages read-only and whether certain features should be exposed to a given user. The capability of sharing objects with specific users is built in, but is not enabled from the frontend at the moment. Allowing such widespread sharing and user lookups could be a security hole and could also unduly tax the system. We'd like to test these capabilities more fully before letting users used them.</p>"},{"location":"walkthrough/key-concepts/#graphql","title":"GraphQL","text":""},{"location":"walkthrough/key-concepts/#mutations-and-queries","title":"Mutations and Queries","text":"<p>OpenContracts uses Graphene and GraphQL to serve data to its frontend. You can access the Graphiql playground by going to your OpenContracts root url <code>/graphql</code> - e.g. <code>https://opencontracts.opensource.legal/graphql</code>. Anonymous users have access to any public data. To authenticate and access your own data, you either need to use the login mutation to create a JWT token or login to the admin dashboard to get a Django session and auth cookie that will automatically authenticate your requests to the GraphQL endpoint.</p> <p>If you're not familiar with GraphQL, it's a very powerful way to expose your backend to the user and/or frontend clients to permit the construction of specific queries with specific data shapes. As an example, here's a request to get public corpuses and the annotated text and labels in them:</p> <p></p> <p>Graphiql comes with a built-in documentation browser. Just click \"Docs\" in the top-right of the screen to start browsing. Typically, mutations change things on the server. Queries merely request copies of data from the server. We've tried to make our schema fairly self-explanatory, but we do plan to add more descriptions and guidance to our API docs.</p>"},{"location":"walkthrough/key-concepts/#graphql-only-features","title":"GraphQL-only features","text":"<p>Some of our features are currently not accessible via the frontend. Sharing analyses and corpuses to the public, for example, can only be achieved via <code>makeCorpusPublic</code> and <code>makeAnalysisPublic</code> mutations, and only admins have this power at the moment. For our current release, we've done this to prevent large numbers of public corpuses being shared to cut down on server usage. We'd like to make a fully free and open, collaborative platform with more features to share anonymously, but this will require additional effort and compute power.</p>"},{"location":"walkthrough/step-1-add-documents/","title":"Step 1 - Add Documents","text":"<p>In order to do anything, you need to add some documents to Gremlin.</p>"},{"location":"walkthrough/step-1-add-documents/#go-to-the-documents-tab","title":"Go to the Documents tab","text":"<p>Click on the \"Documents\" entry in the menu to bring up a view of all documents you have read and/or write access to:</p> <p></p>"},{"location":"walkthrough/step-1-add-documents/#open-the-action-menu","title":"Open the Action Menu","text":"<p>Now, click on the \"Action\" dropdown to open the Action menu for available actions and click \"Import\":</p> <p></p> <p>This will bring up a dialog to load documents:</p> <p></p>"},{"location":"walkthrough/step-1-add-documents/#select-documents-to-upload","title":"Select Documents to Upload","text":"<p>Open Contracts works with PDFs only (as this helps us have a single file type with predictable data structures, formats, etc.). In the future, we'll add functionality to convert other files to PDF, but, for now, please use PDFs. It doesn't matter if they are OCRed or not as OpenContracts performs its own OCR on every PDF anyway to ensure consistent OCR quality and outputs. Once you've added documents for upload, you'll see a list of documents:</p> <p></p> <p>Click on a document to change the description or title:</p> <p></p>"},{"location":"walkthrough/step-1-add-documents/#upload-your-documents","title":"Upload Your Documents","text":"<p>Click upload to upload the documents to OpenContracts. Note Once the documents are uploaded, they are automatically processed with Tesseract amd PAWLs to create a layer of tokens - each one representing a word / symbol in the PDF an its X,Y coordinates on the page. This is what powers OpenContracts annotator and allows us to create both layout-aware and text-only annotations. While the PAWLs processing script is running, the document you uploaded will not be available for viewing and cannot be added to a corpus. You'll see a loading bar on the document until the pre-processing is complete. This is only one once and can take a long time (a couple of minutes to a max of 10) depending on the document length, quality, etc.</p> <p></p>"},{"location":"walkthrough/step-2-create-labelset/","title":"Step 2 - Create Labelset","text":""},{"location":"walkthrough/step-2-create-labelset/#why-labelsets","title":"Why Labelsets?","text":"<p>Before you can add labels, you need to decide what you want to label. A labelset should reflect the taxonomy or concepts you want to associate with text in your document. This can be solely for the purpose of human review and retrieval, but we imagine many of you want to use it to train machine learning models.</p> <p>At the moment, there's no way to create a label in a corpus without creating a labelset and creating a label for the labelset (though we'd like to add that and welcome contributions).</p>"},{"location":"walkthrough/step-2-create-labelset/#create-text-labels","title":"Create Text Labels","text":"<p>Let's say we want to add some labels for \"Parties\", \"Termination Clause\", and \"Effective Date\". To do that, let's first create a LabelSet to hold the labels.</p> <ol> <li>Go to the labelset view and click the action button to bring up the action menu:    </li> <li>Clicking on the \"Create Label Set\" item will bring up a modal to let you create labels:    </li> <li>Now click on the new label set to edit the labels:    </li> <li> <p>A modal comes up that lets you edit three types of labels:</p> <ol> <li>Text Labels - are meant to label spans of text (\"highlights\")</li> <li>Relationship Labels - this feature is still under development, but it labels relationships bewteen text label     (e.g. one labelled party is the \"Parent Company\" of another).</li> <li>Doc Type Labels - are meant to label what category the document belongs in - e.g. a \"Stock Purchase Agreement\"      or an \"NDA\"</li> </ol> </li> <li> <p>Click the \"Text Labels\" tab to bring up a view of current labels for text annotations and an action    button that lets you create new ones. There should be no labels when you first open this view\"    </p> </li> <li>Click the action button and then the \"Create Text Label\" dropdown item:    </li> <li>You'll see a new, blank label in the list of text labels:    </li> <li>Click the edit icon on the label to edit the label title, description, color    and/or icon. To edit the icon or highlight color, hover over or click the giant    tag icon on the left side of the label:    </li> <li>Hit save to commit the changes to the database. Repeat for the other labels - \"Parties\",    \"Termination Clause\", and \"Effective Date\":    </li> </ol>"},{"location":"walkthrough/step-2-create-labelset/#create-document-type-labels","title":"Create Document-Type Labels","text":"<p>In addition to labelling specific parts of a document, you may want to tag a document itself as a certain type of document or addressing a certain subject. In this example, let's say we want to label some documents as \"contracts\" and others as \"not contracts\".</p> <ol> <li>Let's also create two example document type labels. Click the \"Doc Type Labels\" tab:    </li> <li>As before, click the action button and the \"Create Document Type Label\" item to create a    blank document type label:    </li> <li>Repeat to create two doc type labels - \"Contract\" and \"Not Contract\":    </li> <li>Hit \"Close\" to close the editor.</li> </ol>"},{"location":"walkthrough/step-3-create-a-corpus/","title":"Step 3 - Create Corpus","text":""},{"location":"walkthrough/step-3-create-a-corpus/#purpose-of-the-corpus","title":"Purpose of the Corpus","text":"<p>A \"Corpus\" is a collection of documents that can be annotated by hand or automatically by a \"Gremlin\" analyzer. In order to create a Corpus, you first need to create a Corpus and then add documents to it.</p>"},{"location":"walkthrough/step-3-create-a-corpus/#go-to-the-corpus-page","title":"Go to the Corpus Page","text":"<ol> <li>First, login if you're not already logged in.</li> <li>Then, go the \"Corpus\" tab and click the \"Action\" dropdown to bring up    the action menu:    </li> <li>Click \"Create Corpus\" to bring up the Create Corpus dialog. If you've already created a labelset or have a    pre-existing one, you can select it, otherwise you'll need to create and add one later:    </li> <li>Assuming you created the labelset you want to use, when you click on the dropdown in the \"Label Set\" section, you    should see your new labelset. Click on it to select it:    </li> <li>You will now be able to open the corpus again, open documents in the corpus and start labelling.</li> </ol>"},{"location":"walkthrough/step-3-create-a-corpus/#add-documents-to-corpus","title":"Add Documents to Corpus","text":"<ol> <li>Once you have a corpus, go back to the document page to select documents to add. You can do this in one of two ways.<ol> <li>Right-click on a document to show a context menu:       </li> <li>Or, SHIFT + click on the documents you want to select in order to select multiple documents at once. A green       checkmark will appear on selected documents.       </li> </ol> </li> <li>When you're done, click the \"Action\"    </li> <li>A dialog will pop up asking you to select a corpus to add the documents to. Select the desired corpus and    hit ok.    </li> <li>You'll get a confirmation dialog. Hit OK.</li> <li>When you click on the Corpus you just added the documents to, you'll get a tabbed view of all of the    documents, annotations and analyses for that Corpus. At this stage, you should see your documents:    </li> </ol> <p>Congrats! You've created a corpus to hold annotations or perform an analysis! In order to start labelling it yourself, you need to create and then select a LabelSet, however. You do not need to do this to run an analyzer, however.</p> <p>Note: If you have an OpenContracts export file and proper permissions, you can also import a corpus, documents, annotations, and labels. This is disabled on our demo instance, however, to but down on server load and reduce opportunities to upload potentially malicious files. See the \"Advanced\" section for more details.</p>"},{"location":"walkthrough/step-4-create-text-annotations/","title":"Step 4 - Create Some Annotations","text":"<p>To view or edit annotations, you need to open a corpus and then open a document in the Corpus.</p> <ol> <li>Go to your Corpuses page and click on the corpus you just created:</li> <li>This will open up the document view again. Click on one of the documents to bring up the annotator:    </li> <li>To select the label to apply, Click the vertical ellipses in the \"Text Label to Apply Widget\". This    will bring up an interface that lets you search your labelset and select a label:    </li> <li>Select the \"Effective Date\" label, for example, to label the Effective Date:    </li> <li>Now, in the document, click and drag a box around the language that corresponds to    your select label:    </li> <li>When you've selected the correct text, release the mouse. You'll see a confirmtion when your annotation    is created (you'll also see the annotation in the sidebar to the left):    </li> <li>If you want to delete the annotation, you can click on the trash icon in the corresponding annotation card in the    sidebar, or, when you hover over the annotation on the page, you'll see a trash icon in the label bar of the    annotation. You can click this to delete the annotation too.    </li> <li>If your desired annotated text is non-contiguous, you can hold down the SHIFT key while selecting blocks of text    to combine them into a single annotation. While holding SHIFT, releasing the mouse will not create the annotation in    the database, it will just allow you to move to a new area.<ol> <li>One situation you might want to do this is where what you want to highlight is on different lines but is just a       small part of the surrounding paragraph (such as this example, where Effective Date spans two lines):       </li> <li>Or you might want to select multiple snippets of text in a larger block of text, such as where you have multiple       parties you want to combine into a single annotation:       </li> </ol> </li> </ol>"},{"location":"walkthrough/step-5-create-doc-type-annotations/","title":"Step 5 - Create Some Document Annotations","text":"<ol> <li>If you want to label the type of document instead of the text inside it, use the controls in the \"Doc Type\"    widget on the bottom right of the Annotator. Hover over it and a green plus button should appear:    </li> <li>Click the \"+\" button to bring up a dialog that lets you search and select document type labels (remember, we created    these earlier in the tutorial):    </li> <li>Click \"Add Label\" to actually apply the label, and you'll now see that label displayed in the \"Doc Type\"    widget in the annotator:    </li> <li>As before, you can click the trash can to delete the label.</li> </ol>"},{"location":"walkthrough/step-6-search-and-filter-by-annotations/","title":"Step 6 - Search and Filter By Annotations","text":"<ol> <li>Back in the Corpus view, you can see in the document view the document type label you just added:    </li> <li>You can click on the filter dropdown above to filter the documents to only those with a certain doc type label:    </li> <li>With the corpus opened, click on the \"Annotations\" tab instead of the \"Documents\" tab to get a summary    of all the current annotations in the Corpus:    </li> <li>Click on an annotation card to automatically load the document it's in and jump right to the page containing the    annotation:    </li> </ol>"},{"location":"walkthrough/advanced/configure-annotation-view/","title":"Configure How Annotations Are Displayed","text":"<p>Annotations are composed of tokens (basically text in a line surrounded by whitespace). The tokens have a highlight. OpenContracts also has a \"BoundingBox\" around the tokens which is the smallest rectangle that can cover all of the tokens in an Annotation.</p> <p>In the Annotator view, you'll see a purple-colored \"eye\" icon in the top left of the annotation list in the sidebar. Click the icon to bring up a series of configurations for how annotations are displayed:</p> <p></p> <p>There are three different settings that can be combined to significantly change how you see the annotations: 1. Show only selected - You will only see the annotation selected, either by clicking on it in the sidebar or when you    clicked into an annotation from the Corpus view. All other annotations will be completely hidden. 2. Show bounding boxes - If you unselect this, only the tokens will be visible. This is recommended where you large    numbers of overlapping annotations or annotations that are sparse - e.g. a few words scattered throughout a paragraph.    In either of these cases, the bounding boxes can cover other bounding boxes and this can be confusing. Where you have    too many overlapping bounding boxes, it's easier to hide them and just look at the tokens. 3. Label Display Behavior - has three options:</p> <ol> <li>Always Show - Always show the label for an annotation when it's displayed (remember, you can choose to only           display selected annotations).</li> <li>Always Hide - Never show the label for an annotation, regardless of its visiblity.</li> <li>Show on Hover - If an annotation is visible, when you hover over it, you'll see the label.</li> </ol>"},{"location":"walkthrough/advanced/export-import-corpuses/","title":"Import and Export Corpuses","text":""},{"location":"walkthrough/advanced/export-import-corpuses/#exports","title":"Exports","text":"<p>OpenContracts support both exporting and importing corpuses. This functionality is disabled on the public demo as it can be bandwidth intensive. If you want to experiment with these features on your own, you'll see the export action when you right-click on a corpus:</p> <p></p> <p>You can access your exports from the user dropdown menu in the top right corner of the screen. Once your export is complete, you should be able to download a zip containing all the documents, their PAWLs layers, and the corpus data you created - including all annotations.</p>"},{"location":"walkthrough/advanced/export-import-corpuses/#imports","title":"Imports","text":"<p>If you've enabled corpus imports (see the frontend env file for the boolean toggle to do this - it's <code>REACT_APP_ALLOW_IMPORTS</code>), you'll see an import action when you click the action button on the corpus page.</p>"},{"location":"walkthrough/advanced/fork-a-corpus/","title":"Fork a Corpus","text":""},{"location":"walkthrough/advanced/fork-a-corpus/#to-fork-or-not-to-fork","title":"To Fork or Not to Fork?","text":"<p>One of the amazing things about Open Source collaboration is you can stand on the shoulder of giants - we can share techniques and data and collectively achieve what we could never do alone. OpenContracts is designed to make it super easy to share and re-use annotation data.</p> <p>In OpenContracts, we introduce the concept of \"forking\" a corpus - basically creating a copy of public or private corpus, complete with its documents and annotations, which you can edit and tweak as needed. This opens up some interesting possibilities. For example, you might have a base corpus with annotations common to many types of AI models or annotation projects which you can fork as needed and layer task or domain-specific annotations on top of.</p>"},{"location":"walkthrough/advanced/fork-a-corpus/#fork-a-corpus","title":"Fork a Corpus","text":"<p>Forking a corpus is easy.</p> <ol> <li>Again, right-click on a corpus to bring up the context menu. You'll see an entry to \"Fork Corpus\":    </li> <li>Click on it to start a fork. You should see a confirmation in the top right of the screen:    </li> <li>Once the fork is complete, the next time you go to your Corpus page, you'll see a new Corpus with a Fork    icon in the icon bar at the bottom. If you hover over it, you'll be able to see a summary of the corpus it was    forked from. This is tracked in the database, so, long-term, we'd like to have corpus version control similar to how    git works:    </li> </ol>"},{"location":"walkthrough/advanced/generate-graphql-schema-files/","title":"Generate GraphQL Schema Files","text":""},{"location":"walkthrough/advanced/generate-graphql-schema-files/#generating-graphql-schema-files","title":"Generating GraphQL Schema Files","text":"<p>Open Contracts uses Graphene to provide a rich GraphQL endpoint, complete with the GraphiQL query application. For some applications, you may want to generate a GraphQL schema file in SDL or json. On example use case is if you're developing a frontend you want to connect to OpenContracts, and you'd like to autogenerate Typescript types from a GraphQL Schena.</p> <p>To generate a GraphQL schema file, run your choice of the following commands.</p> <p>For an SDL file:</p> <pre><code>$ docker-compose -f local.yml run django python manage.py graphql_schema --schema config.graphql.schema.schema --out schema.graphql\n</code></pre> <p>For a JSON file:</p> <pre><code>$ docker-compose -f local.yml run django python manage.py graphql_schema --schema config.graphql.schema.schema --out schema.json\n</code></pre> <p>You can convert these to TypeScript for use in a frontend (though you'll find this has already been done for the React- based OpenContracts frontend) using a tool like this.</p>"},{"location":"walkthrough/advanced/run-gremlin-analyzer/","title":"Run a Gremlin Analyzer","text":""},{"location":"walkthrough/advanced/run-gremlin-analyzer/#introduction-to-gremlin-integration","title":"Introduction to Gremlin Integration","text":"<p>OpenContracts integrates with a powerful NLP engine called Gremlin Engine (\"Gremlin\"). If you run a Gremlin analyzer on a Corpus, it will create annotations of its own that you can view and export (e.g. automatically applying document labels or labeling parties, dates, and places, etc.). It's meant to provide a consistent API to deliver and render NLP and machine learning capabilities to end-users. As discussed in the configuration section, you need to install Gremlin Analyzers through the admin dashboard.</p> <p>Once you've installed Gremlin Analyzers, however, it's easy to apply them.</p>"},{"location":"walkthrough/advanced/run-gremlin-analyzer/#using-an-installed-gremlin-analyzer","title":"Using an Installed Gremlin Analyzer","text":"<ol> <li> <p>If analysis capabilities are enabled for instance, when you right-click on a Corpus, you'll see an option to    \"Analyze Corpus\":   </p> </li> <li> <p>Clicking on this item will bring up a dialog where you can browse available analyzers:   </p> </li> <li> <p>Select one and hit \"Analyze\" to submit a corpus for processing. When you go to the Analysis tab of your    Corpus now, you'll see the analysis. Most likely, if you just clicked there, it will say processing:    </p> </li> <li> <p>When the Analysis is complete, you'll see a summary of the number of labels and annotations applied by the analyzer:    </p> </li> </ol>"},{"location":"walkthrough/advanced/run-gremlin-analyzer/#note-on-processing-time","title":"Note on Processing Time","text":"<p>Large Corpuses of hundreds of documents can take a long time to process (10 minutes or more). It's hard to predict processing time up front, because it's dependent on the number of total pages and the specific analysis being performed. At the moment, there is not a great mechanism in place to detect and handle failures in a Gremlin analyzer and reflect this in OpenContracts. It's on our roadmap to improve this integration. In the meantime, the example analyzers we've released with Gremlin should be very stable, so they should run predictably.</p>"},{"location":"walkthrough/advanced/run-gremlin-analyzer/#viewing-the-outputs","title":"Viewing the Outputs","text":"<p>Once an Analysis completes, you'll be able to browse the annotations from the analysis in several ways.</p> <ol> <li>First, they'll be available in the \"Annotation\" tab, and you can easily filter to annotations from a    specific analyzer.</li> <li>Second, when you load a Document, in the Annotator view, there's a small widget in the top of the annotator    that has three downwards-facing arrows and says \"Human Annotation Mode\".    </li> <li>Click on the arrows open a tray showing the analyses applied to this document.</li> <li>Click on an analysis to load the annotations and view them in the document.    </li> </ol> <p>Note: You can delete an analysis, but you cannot edit it. The annotations are machine-created and cannot be edited by human users.</p>"}]}